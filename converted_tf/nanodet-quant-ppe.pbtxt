name: "Sony"
layer {
  name: "Placeholder: input_1"
  type: "Placeholder"
  
  top: "input_1:0"
    scale_param {
    "InputShapes": ""
    "OutputShape": "416x416x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "511.00KB"
    "ScheduleId": "319"

  }

}
layer {
  name: "Pad: keras_quantization_wrapper_pad_to1d_0"
  type: "Pad"
  bottom: "input_1:0"
  top: "keras_quantization_wrapper_pad_to1d_0:0"
    scale_param {
    "InputShapes": "416x416x3"
    "OutputShape": "418x416x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "509.44KB"
    "ScheduleId": "320"

  }

}
layer {
  name: "Transform: transform-59-keras_quantization_wrapper_pad_to1d_0"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pad_to1d_0:0"
  top: "transform-59-keras_quantization_wrapper_pad_to1d_0:0"
    scale_param {
    "InputShapes": "418x416x3"
    "OutputShape": "418x416x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "546.00KB"
    "ScheduleId": "322"

  }

}
layer {
  name: "Pad: keras_quantization_wrapper_pad_to1d_1"
  type: "Pad"
  bottom: "transform-59-keras_quantization_wrapper_pad_to1d_0:0"
  top: "keras_quantization_wrapper_pad_to1d_1:0"
    scale_param {
    "InputShapes": "418x416x3"
    "OutputShape": "418x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "323"

  }

}
layer {
  name: "Transform: transform-61-keras_quantization_wrapper_pad_to1d_1"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pad_to1d_1:0"
  top: "transform-61-keras_quantization_wrapper_pad_to1d_1:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "418x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "325"

  }

}
layer {
  name: "Transform: transform-63-keras_quantization_wrapper_pad_to1d_1"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pad_to1d_1:0"
  top: "transform-63-keras_quantization_wrapper_pad_to1d_1:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "418x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "329"

  }

}
layer {
  name: "Transform: transform-65-keras_quantization_wrapper_pad_to1d_1"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pad_to1d_1:0"
  top: "transform-65-keras_quantization_wrapper_pad_to1d_1:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "418x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "333"

  }

}
layer {
  name: "Transform: transform-67-keras_quantization_wrapper_pad_to1d_1"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pad_to1d_1:0"
  top: "transform-67-keras_quantization_wrapper_pad_to1d_1:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "418x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "337"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(0;0)_dim_0"
  type: "StrideSlice"
  bottom: "transform-61-keras_quantization_wrapper_pad_to1d_1:0"
  top: "keras_quantization_wrapper_pf(0;0)_dim_0:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "326"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(0;1)_dim_0"
  type: "StrideSlice"
  bottom: "transform-63-keras_quantization_wrapper_pad_to1d_1:0"
  top: "keras_quantization_wrapper_pf(0;1)_dim_0:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "330"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(1;0)_dim_0"
  type: "StrideSlice"
  bottom: "transform-65-keras_quantization_wrapper_pad_to1d_1:0"
  top: "keras_quantization_wrapper_pf(1;0)_dim_0:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "334"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(1;1)_dim_0"
  type: "StrideSlice"
  bottom: "transform-67-keras_quantization_wrapper_pad_to1d_1:0"
  top: "keras_quantization_wrapper_pf(1;1)_dim_0:0"
    scale_param {
    "InputShapes": "418x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "338"

  }

}
layer {
  name: "Transform: transform-60-keras_quantization_wrapper_pf(0;0)_dim_0"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pf(0;0)_dim_0:0"
  top: "transform-60-keras_quantization_wrapper_pf(0;0)_dim_0:0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "327"

  }

}
layer {
  name: "Transform: transform-62-keras_quantization_wrapper_pf(0;1)_dim_0"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pf(0;1)_dim_0:0"
  top: "transform-62-keras_quantization_wrapper_pf(0;1)_dim_0:0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "331"

  }

}
layer {
  name: "Transform: transform-64-keras_quantization_wrapper_pf(1;0)_dim_0"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pf(1;0)_dim_0:0"
  top: "transform-64-keras_quantization_wrapper_pf(1;0)_dim_0:0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "335"

  }

}
layer {
  name: "Transform: transform-66-keras_quantization_wrapper_pf(1;1)_dim_0"
  type: "Transform"
  bottom: "keras_quantization_wrapper_pf(1;1)_dim_0:0"
  top: "transform-66-keras_quantization_wrapper_pf(1;1)_dim_0:0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x418x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "274.31KB"
    "ScheduleId": "339"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(0;0)"
  type: "StrideSlice"
  bottom: "transform-60-keras_quantization_wrapper_pf(0;0)_dim_0:0"
  top: "keras_quantization_wrapper_pf(0;0):0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x209x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "137.16KB"
    "ScheduleId": "328"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(0;1)"
  type: "StrideSlice"
  bottom: "transform-62-keras_quantization_wrapper_pf(0;1)_dim_0:0"
  top: "keras_quantization_wrapper_pf(0;1):0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x209x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "137.16KB"
    "ScheduleId": "332"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(1;0)"
  type: "StrideSlice"
  bottom: "transform-64-keras_quantization_wrapper_pf(1;0)_dim_0:0"
  top: "keras_quantization_wrapper_pf(1;0):0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x209x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "137.16KB"
    "ScheduleId": "336"

  }

}
layer {
  name: "StrideSlice: keras_quantization_wrapper_pf(1;1)"
  type: "StrideSlice"
  bottom: "transform-66-keras_quantization_wrapper_pf(1;1)_dim_0:0"
  top: "keras_quantization_wrapper_pf(1;1):0"
    scale_param {
    "InputShapes": "209x418x3"
    "OutputShape": "209x209x3"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "137.16KB"
    "ScheduleId": "340"

  }

}
layer {
  name: "Concat: keras_quantization_wrapper_concat"
  type: "Concat"
  bottom: "keras_quantization_wrapper_pf(1;0):0"
  bottom: "keras_quantization_wrapper_pf(0;1):0"
  bottom: "keras_quantization_wrapper_pf(1;1):0"
  bottom: "keras_quantization_wrapper_pf(0;0):0"
  top: "keras_quantization_wrapper_concat:0"
    scale_param {
    "InputShapes": "209x209x3, 209x209x3, 209x209x3, 209x209x3"
    "OutputShape": "209x209x12"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "341"

  }

}
layer {
  name: "Transform: transform-3-keras_quantization_wrapper_concat"
  type: "Transform"
  bottom: "keras_quantization_wrapper_concat:0"
  top: "transform-3-keras_quantization_wrapper_concat:0"
    scale_param {
    "InputShapes": "209x209x12"
    "OutputShape": "209x209x12"
    "Quantize[mn,mx]": [-4.0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "548.63KB"
    "ScheduleId": "342"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper"
  type: "Convolution"
  bottom: "transform-3-keras_quantization_wrapper_concat:0"
  top: "leaky_re_lu:0"
    scale_param {
    "InputShapes": "209x209x12, 2x2x12x24, 24"
    "OutputShape": "208x208x24"
    "Quantize[mn,mx]": [-1024.0, 1024.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_kernel_pf:0"
    "ConstInputs": "keras_quantization_wrapper_bias:0"
    "ConstInputs": "leaky_re_lu_slope:0"
    "RuntimeMemory": "1.32MB"
    "ScheduleId": "343"

  }
  blobs {
    shape {
      dim: 2
      dim: 2
      dim: 12
      dim: 24
    }
  }
  blobs {
    shape {
      dim: 24
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu"
  type: "Prelu"
  bottom: "leaky_re_lu:0"
  top: "leaky_re_lu:0"
    scale_param {
    "InputShapes": "208x208x24, 1"
    "OutputShape": "208x208x24"
    "Quantize[mn,mx]": [-64.0, 64.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_kernel_pf:0"
    "ConstInputs": "keras_quantization_wrapper_bias:0"
    "ConstInputs": "leaky_re_lu_slope:0"
    "RuntimeMemory": "1.32MB"
    
  }

}
layer {
  name: "Pad: zero_padding2d_1_to1d_0"
  type: "Pad"
  bottom: "leaky_re_lu:0"
  top: "zero_padding2d_1_to1d_0:0"
    scale_param {
    "InputShapes": "208x208x24"
    "OutputShape": "209x208x24"
    "Quantize[mn,mx]": [-64.0, 64.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "1.33MB"
    "ScheduleId": "344"

  }

}
layer {
  name: "Pad: zero_padding2d_1_to1d_1"
  type: "Pad"
  bottom: "zero_padding2d_1_to1d_0:0"
  top: "zero_padding2d_1_to1d_1:0"
    scale_param {
    "InputShapes": "209x208x24"
    "OutputShape": "209x209x24"
    "Quantize[mn,mx]": [-64.0, 64.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "1.33MB"
    "ScheduleId": "346"

  }

}
layer {
  name: "MaxPool: maxpool1"
  type: "MaxPool"
  bottom: "zero_padding2d_1_to1d_1:0"
  top: "maxpool1:0"
    scale_param {
    "InputShapes": "209x209x24"
    "OutputShape": "104x104x24"
    "Quantize[mn,mx]": [-64.0, 64.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "348"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_1"
  type: "Convolution"
  bottom: "maxpool1:0"
  top: "backbone.stage2.0.branch2.2:0"
    scale_param {
    "InputShapes": "104x104x24, 1x1x24x88, 88"
    "OutputShape": "104x104x88"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_1_kernel:0"
    "ConstInputs": "backbone.stage2.0.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_1_bias:0"
    "RuntimeMemory": "1014.00KB"
    "ScheduleId": "349"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 24
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.0.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage2.0.branch2.2:0"
  top: "backbone.stage2.0.branch2.2:0"
    scale_param {
    "InputShapes": "104x104x88, 1"
    "OutputShape": "104x104x88"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_1_kernel:0"
    "ConstInputs": "backbone.stage2.0.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_1_bias:0"
    "RuntimeMemory": "1014.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_2"
  type: "Convolution"
  bottom: "maxpool1:0"
  top: "keras_quantization_wrapper_2:0"
    scale_param {
    "InputShapes": "104x104x24, 3x3x1x24, 24"
    "OutputShape": "52x52x24"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_2_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_2_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "350"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 24
    }
  }
  blobs {
    shape {
      dim: 24
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_4"
  type: "Convolution"
  bottom: "backbone.stage2.0.branch2.2:0"
  top: "keras_quantization_wrapper_4:0"
    scale_param {
    "InputShapes": "104x104x88, 3x3x1x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-64.0, 64.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_4_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_4_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "353"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_3"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_2:0"
  top: "backbone.stage2.0.branch1.4:0"
    scale_param {
    "InputShapes": "52x52x24, 1x1x24x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-512.0, 512.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_3_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_3_bias:0"
    "ConstInputs": "backbone.stage2.0.branch1.4_slope:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "352"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 24
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.0.branch1.4"
  type: "Prelu"
  bottom: "backbone.stage2.0.branch1.4:0"
  top: "backbone.stage2.0.branch1.4:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_3_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_3_bias:0"
    "ConstInputs": "backbone.stage2.0.branch1.4_slope:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_5"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_4:0"
  top: "backbone.stage2.0.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-512.0, 512.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_5_kernel:0"
    "ConstInputs": "backbone.stage2.0.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_5_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "355"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.0.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage2.0.branch2.7:0"
  top: "backbone.stage2.0.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_5_kernel:0"
    "ConstInputs": "backbone.stage2.0.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_5_bias:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Concat: backbone.stage2.0/concat_2"
  type: "Concat"
  bottom: "backbone.stage2.0.branch2.7:0"
  bottom: "backbone.stage2.0.branch1.4:0"
  top: "backbone.stage2.0/concat_2:0"
    scale_param {
    "InputShapes": "52x52x88, 52x52x88"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "356"

  }

}
layer {
  name: "Transform: transform-94-backbone.stage2.0/concat_2"
  type: "Transform"
  bottom: "backbone.stage2.0/concat_2:0"
  top: "transform-94-backbone.stage2.0/concat_2:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "357"

  }

}
layer {
  name: "Reshape: tf.reshape"
  type: "Reshape"
  bottom: "transform-94-backbone.stage2.0/concat_2:0"
  top: "tf.reshape:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "358"

  }

}
layer {
  name: "Transform: transform-72-tf.reshape"
  type: "Transform"
  bottom: "tf.reshape:0"
  top: "transform-72-tf.reshape:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "360"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose"
  type: "Transpose"
  bottom: "transform-72-tf.reshape:0"
  top: "tf.compat.v1.transpose:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "361"

  }

}
layer {
  name: "Transform: transform-105-tf.compat.v1.transpose"
  type: "Transform"
  bottom: "tf.compat.v1.transpose:0"
  top: "transform-105-tf.compat.v1.transpose:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "362"

  }

}
layer {
  name: "Reshape: tf.reshape_1"
  type: "Reshape"
  bottom: "transform-105-tf.compat.v1.transpose:0"
  top: "tf.reshape_1:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "363"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem"
  type: "StrideSlice"
  bottom: "tf.reshape_1:0"
  top: "tf.__operators__.getitem:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "365"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_1"
  type: "StrideSlice"
  bottom: "tf.reshape_1:0"
  top: "tf.__operators__.getitem_1:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "366"

  }

}
layer {
  name: "Transform: transform-35-tf.__operators__.getitem"
  type: "Transform"
  bottom: "tf.__operators__.getitem:0"
  top: "transform-35-tf.__operators__.getitem:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "373"

  }

}
layer {
  name: "Transform: transform-30-tf.__operators__.getitem_1"
  type: "Transform"
  bottom: "tf.__operators__.getitem_1:0"
  top: "transform-30-tf.__operators__.getitem_1:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "367"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_6"
  type: "Convolution"
  bottom: "transform-30-tf.__operators__.getitem_1:0"
  top: "backbone.stage2.1.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-512.0, 512.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_6_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_6_bias:0"
    "ConstInputs": "backbone.stage2.1.branch2.2_slope:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "368"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.1.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage2.1.branch2.2:0"
  top: "backbone.stage2.1.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_6_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_6_bias:0"
    "ConstInputs": "backbone.stage2.1.branch2.2_slope:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_7"
  type: "Convolution"
  bottom: "backbone.stage2.1.branch2.2:0"
  top: "keras_quantization_wrapper_7:0"
    scale_param {
    "InputShapes": "52x52x88, 3x3x1x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-64.0, 64.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_7_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_7_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "369"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_8"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_7:0"
  top: "backbone.stage2.1.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-512.0, 512.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_8_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_8_bias:0"
    "ConstInputs": "backbone.stage2.1.branch2.7_slope:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "371"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.1.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage2.1.branch2.7:0"
  top: "backbone.stage2.1.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_8_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_8_bias:0"
    "ConstInputs": "backbone.stage2.1.branch2.7_slope:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Transform: transform-29-keras_quantization_wrapper_8"
  type: "Transform"
  bottom: "backbone.stage2.1.branch2.7:0"
  top: "transform-29-keras_quantization_wrapper_8:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "372"

  }

}
layer {
  name: "Concat: backbone.stage2.1/concat_1"
  type: "Concat"
  bottom: "transform-35-tf.__operators__.getitem:0"
  bottom: "transform-29-keras_quantization_wrapper_8:0"
  top: "backbone.stage2.1/concat_1:0"
    scale_param {
    "InputShapes": "52x52x88, 52x52x88"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "374"

  }

}
layer {
  name: "Transform: transform-116-backbone.stage2.1/concat_1"
  type: "Transform"
  bottom: "backbone.stage2.1/concat_1:0"
  top: "transform-116-backbone.stage2.1/concat_1:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "375"

  }

}
layer {
  name: "Reshape: tf.reshape_2"
  type: "Reshape"
  bottom: "transform-116-backbone.stage2.1/concat_1:0"
  top: "tf.reshape_2:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "376"

  }

}
layer {
  name: "Transform: transform-79-tf.reshape_2"
  type: "Transform"
  bottom: "tf.reshape_2:0"
  top: "transform-79-tf.reshape_2:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "378"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_1"
  type: "Transpose"
  bottom: "transform-79-tf.reshape_2:0"
  top: "tf.compat.v1.transpose_1:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "379"

  }

}
layer {
  name: "Transform: transform-124-tf.compat.v1.transpose_1"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_1:0"
  top: "transform-124-tf.compat.v1.transpose_1:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "380"

  }

}
layer {
  name: "Reshape: tf.reshape_3"
  type: "Reshape"
  bottom: "transform-124-tf.compat.v1.transpose_1:0"
  top: "tf.reshape_3:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "381"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_2"
  type: "StrideSlice"
  bottom: "tf.reshape_3:0"
  top: "tf.__operators__.getitem_2:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "383"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_3"
  type: "StrideSlice"
  bottom: "tf.reshape_3:0"
  top: "tf.__operators__.getitem_3:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "384"

  }

}
layer {
  name: "Transform: transform-36-tf.__operators__.getitem_2"
  type: "Transform"
  bottom: "tf.__operators__.getitem_2:0"
  top: "transform-36-tf.__operators__.getitem_2:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "391"

  }

}
layer {
  name: "Transform: transform-28-tf.__operators__.getitem_3"
  type: "Transform"
  bottom: "tf.__operators__.getitem_3:0"
  top: "transform-28-tf.__operators__.getitem_3:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "385"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_9"
  type: "Convolution"
  bottom: "transform-28-tf.__operators__.getitem_3:0"
  top: "backbone.stage2.2.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_9_kernel:0"
    "ConstInputs": "backbone.stage2.2.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_9_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "386"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.2.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage2.2.branch2.2:0"
  top: "backbone.stage2.2.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_9_kernel:0"
    "ConstInputs": "backbone.stage2.2.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_9_bias:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_10"
  type: "Convolution"
  bottom: "backbone.stage2.2.branch2.2:0"
  top: "keras_quantization_wrapper_10:0"
    scale_param {
    "InputShapes": "52x52x88, 3x3x1x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_10_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_10_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "387"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_11"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_10:0"
  top: "backbone.stage2.2.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-512.0, 512.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_11_kernel:0"
    "ConstInputs": "backbone.stage2.2.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_11_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "389"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.2.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage2.2.branch2.7:0"
  top: "backbone.stage2.2.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_11_kernel:0"
    "ConstInputs": "backbone.stage2.2.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_11_bias:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Transform: transform-27-keras_quantization_wrapper_11"
  type: "Transform"
  bottom: "backbone.stage2.2.branch2.7:0"
  top: "transform-27-keras_quantization_wrapper_11:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "390"

  }

}
layer {
  name: "Concat: backbone.stage2.2/concat_1"
  type: "Concat"
  bottom: "transform-27-keras_quantization_wrapper_11:0"
  bottom: "transform-36-tf.__operators__.getitem_2:0"
  top: "backbone.stage2.2/concat_1:0"
    scale_param {
    "InputShapes": "52x52x88, 52x52x88"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "392"

  }

}
layer {
  name: "Transform: transform-128-backbone.stage2.2/concat_1"
  type: "Transform"
  bottom: "backbone.stage2.2/concat_1:0"
  top: "transform-128-backbone.stage2.2/concat_1:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "393"

  }

}
layer {
  name: "Reshape: tf.reshape_4"
  type: "Reshape"
  bottom: "transform-128-backbone.stage2.2/concat_1:0"
  top: "tf.reshape_4:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "394"

  }

}
layer {
  name: "Transform: transform-80-tf.reshape_4"
  type: "Transform"
  bottom: "tf.reshape_4:0"
  top: "transform-80-tf.reshape_4:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "396"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_2"
  type: "Transpose"
  bottom: "transform-80-tf.reshape_4:0"
  top: "tf.compat.v1.transpose_2:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "397"

  }

}
layer {
  name: "Transform: transform-129-tf.compat.v1.transpose_2"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_2:0"
  top: "transform-129-tf.compat.v1.transpose_2:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "398"

  }

}
layer {
  name: "Reshape: tf.reshape_5"
  type: "Reshape"
  bottom: "transform-129-tf.compat.v1.transpose_2:0"
  top: "tf.reshape_5:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "399"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_4"
  type: "StrideSlice"
  bottom: "tf.reshape_5:0"
  top: "tf.__operators__.getitem_4:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "401"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_5"
  type: "StrideSlice"
  bottom: "tf.reshape_5:0"
  top: "tf.__operators__.getitem_5:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "402"

  }

}
layer {
  name: "Transform: transform-37-tf.__operators__.getitem_4"
  type: "Transform"
  bottom: "tf.__operators__.getitem_4:0"
  top: "transform-37-tf.__operators__.getitem_4:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "409"

  }

}
layer {
  name: "Transform: transform-26-tf.__operators__.getitem_5"
  type: "Transform"
  bottom: "tf.__operators__.getitem_5:0"
  top: "transform-26-tf.__operators__.getitem_5:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "403"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_12"
  type: "Convolution"
  bottom: "transform-26-tf.__operators__.getitem_5:0"
  top: "backbone.stage2.3.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_12_kernel:0"
    "ConstInputs": "backbone.stage2.3.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_12_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "404"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.3.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage2.3.branch2.2:0"
  top: "backbone.stage2.3.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_12_kernel:0"
    "ConstInputs": "backbone.stage2.3.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_12_bias:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_13"
  type: "Convolution"
  bottom: "backbone.stage2.3.branch2.2:0"
  top: "keras_quantization_wrapper_13:0"
    scale_param {
    "InputShapes": "52x52x88, 3x3x1x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_13_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_13_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "405"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_14"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_13:0"
  top: "backbone.stage2.3.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1x1x88x88, 88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-512.0, 512.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_14_kernel:0"
    "ConstInputs": "backbone.stage2.3.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_14_bias:0"
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "407"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 88
      dim: 88
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 88
    }
  }
}
layer {
  name: "Prelu: backbone.stage2.3.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage2.3.branch2.7:0"
  top: "backbone.stage2.3.branch2.7:0"
    scale_param {
    "InputShapes": "52x52x88, 1"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_14_kernel:0"
    "ConstInputs": "backbone.stage2.3.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_14_bias:0"
    "RuntimeMemory": "253.50KB"
    
  }

}
layer {
  name: "Transform: transform-25-keras_quantization_wrapper_14"
  type: "Transform"
  bottom: "backbone.stage2.3.branch2.7:0"
  top: "transform-25-keras_quantization_wrapper_14:0"
    scale_param {
    "InputShapes": "52x52x88"
    "OutputShape": "52x52x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "253.50KB"
    "ScheduleId": "408"

  }

}
layer {
  name: "Concat: backbone.stage2.3/concat_1"
  type: "Concat"
  bottom: "transform-37-tf.__operators__.getitem_4:0"
  bottom: "transform-25-keras_quantization_wrapper_14:0"
  top: "backbone.stage2.3/concat_1:0"
    scale_param {
    "InputShapes": "52x52x88, 52x52x88"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "410"

  }

}
layer {
  name: "Transform: transform-130-backbone.stage2.3/concat_1"
  type: "Transform"
  bottom: "backbone.stage2.3/concat_1:0"
  top: "transform-130-backbone.stage2.3/concat_1:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "411"

  }

}
layer {
  name: "Reshape: tf.reshape_6"
  type: "Reshape"
  bottom: "transform-130-backbone.stage2.3/concat_1:0"
  top: "tf.reshape_6:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "412"

  }

}
layer {
  name: "Transform: transform-81-tf.reshape_6"
  type: "Transform"
  bottom: "tf.reshape_6:0"
  top: "transform-81-tf.reshape_6:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x2x88"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "414"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_3"
  type: "Transpose"
  bottom: "transform-81-tf.reshape_6:0"
  top: "tf.compat.v1.transpose_3:0"
    scale_param {
    "InputShapes": "52x52x2x88"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "415"

  }

}
layer {
  name: "Transform: transform-131-tf.compat.v1.transpose_3"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_3:0"
  top: "transform-131-tf.compat.v1.transpose_3:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x88x2"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "416"

  }

}
layer {
  name: "Reshape: tf.reshape_7"
  type: "Reshape"
  bottom: "transform-131-tf.compat.v1.transpose_3:0"
  top: "tf.reshape_7:0"
    scale_param {
    "InputShapes": "52x52x88x2"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "572.00KB"
    "ScheduleId": "417"

  }

}
layer {
  name: "Transform: transform-2-tf.reshape_7"
  type: "Transform"
  bottom: "tf.reshape_7:0"
  top: "transform-2-tf.reshape_7:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "419"

  }

}
layer {
  name: "Transform: transform-24-tf.reshape_7"
  type: "Transform"
  bottom: "tf.reshape_7:0"
  top: "transform-24-tf.reshape_7:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "421"

  }

}
layer {
  name: "Transform: transform-55-tf.reshape_7"
  type: "Transform"
  bottom: "tf.reshape_7:0"
  top: "transform-55-tf.reshape_7:0"
    scale_param {
    "InputShapes": "52x52x176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "426"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_15"
  type: "Convolution"
  bottom: "transform-2-tf.reshape_7:0"
  top: "leaky_re_lu_1:0"
    scale_param {
    "InputShapes": "52x52x176, 1x1x176x128, 128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_15_kernel:0"
    "ConstInputs": "leaky_re_lu_1_slope:0"
    "ConstInputs": "keras_quantization_wrapper_15_bias:0"
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "420"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_1"
  type: "Prelu"
  bottom: "leaky_re_lu_1:0"
  top: "leaky_re_lu_1:0"
    scale_param {
    "InputShapes": "52x52x128, 1"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_15_kernel:0"
    "ConstInputs": "leaky_re_lu_1_slope:0"
    "ConstInputs": "keras_quantization_wrapper_15_bias:0"
    "RuntimeMemory": "338.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_16"
  type: "Convolution"
  bottom: "transform-24-tf.reshape_7:0"
  top: "backbone.stage3.0.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x176, 1x1x176x176, 176"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_16_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_16_bias:0"
    "ConstInputs": "backbone.stage3.0.branch2.2_slope:0"
    "RuntimeMemory": "507.00KB"
    "ScheduleId": "422"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.0.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.0.branch2.2:0"
  top: "backbone.stage3.0.branch2.2:0"
    scale_param {
    "InputShapes": "52x52x176, 1"
    "OutputShape": "52x52x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_16_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_16_bias:0"
    "ConstInputs": "backbone.stage3.0.branch2.2_slope:0"
    "RuntimeMemory": "507.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_17"
  type: "Convolution"
  bottom: "transform-55-tf.reshape_7:0"
  top: "keras_quantization_wrapper_17:0"
    scale_param {
    "InputShapes": "52x52x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_17_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_17_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "427"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_19"
  type: "Convolution"
  bottom: "backbone.stage3.0.branch2.2:0"
  top: "keras_quantization_wrapper_19:0"
    scale_param {
    "InputShapes": "52x52x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_19_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_19_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "423"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_18"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_17:0"
  top: "backbone.stage3.0.branch1.4:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_18_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_18_bias:0"
    "ConstInputs": "backbone.stage3.0.branch1.4_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "429"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.0.branch1.4"
  type: "Prelu"
  bottom: "backbone.stage3.0.branch1.4:0"
  top: "backbone.stage3.0.branch1.4:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_18_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_18_bias:0"
    "ConstInputs": "backbone.stage3.0.branch1.4_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_20"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_19:0"
  top: "backbone.stage3.0.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_20_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_20_bias:0"
    "ConstInputs": "backbone.stage3.0.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "425"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.0.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.0.branch2.7:0"
  top: "backbone.stage3.0.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_20_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_20_bias:0"
    "ConstInputs": "backbone.stage3.0.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Concat: backbone.stage3.0/concat_2"
  type: "Concat"
  bottom: "backbone.stage3.0.branch2.7:0"
  bottom: "backbone.stage3.0.branch1.4:0"
  top: "backbone.stage3.0/concat_2:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "430"

  }

}
layer {
  name: "Transform: transform-132-backbone.stage3.0/concat_2"
  type: "Transform"
  bottom: "backbone.stage3.0/concat_2:0"
  top: "transform-132-backbone.stage3.0/concat_2:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "431"

  }

}
layer {
  name: "Reshape: tf.reshape_8"
  type: "Reshape"
  bottom: "transform-132-backbone.stage3.0/concat_2:0"
  top: "tf.reshape_8:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "432"

  }

}
layer {
  name: "Transform: transform-82-tf.reshape_8"
  type: "Transform"
  bottom: "tf.reshape_8:0"
  top: "transform-82-tf.reshape_8:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "434"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_4"
  type: "Transpose"
  bottom: "transform-82-tf.reshape_8:0"
  top: "tf.compat.v1.transpose_4:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "435"

  }

}
layer {
  name: "Transform: transform-133-tf.compat.v1.transpose_4"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_4:0"
  top: "transform-133-tf.compat.v1.transpose_4:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "436"

  }

}
layer {
  name: "Reshape: tf.reshape_9"
  type: "Reshape"
  bottom: "transform-133-tf.compat.v1.transpose_4:0"
  top: "tf.reshape_9:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "437"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_6"
  type: "StrideSlice"
  bottom: "tf.reshape_9:0"
  top: "tf.__operators__.getitem_6:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "439"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_7"
  type: "StrideSlice"
  bottom: "tf.reshape_9:0"
  top: "tf.__operators__.getitem_7:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "440"

  }

}
layer {
  name: "Transform: transform-38-tf.__operators__.getitem_6"
  type: "Transform"
  bottom: "tf.__operators__.getitem_6:0"
  top: "transform-38-tf.__operators__.getitem_6:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "447"

  }

}
layer {
  name: "Transform: transform-23-tf.__operators__.getitem_7"
  type: "Transform"
  bottom: "tf.__operators__.getitem_7:0"
  top: "transform-23-tf.__operators__.getitem_7:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "441"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_21"
  type: "Convolution"
  bottom: "transform-23-tf.__operators__.getitem_7:0"
  top: "backbone.stage3.1.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_21_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_21_bias:0"
    "ConstInputs": "backbone.stage3.1.branch2.2_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "442"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.1.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.1.branch2.2:0"
  top: "backbone.stage3.1.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_21_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_21_bias:0"
    "ConstInputs": "backbone.stage3.1.branch2.2_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_22"
  type: "Convolution"
  bottom: "backbone.stage3.1.branch2.2:0"
  top: "keras_quantization_wrapper_22:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_22_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_22_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "443"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_23"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_22:0"
  top: "backbone.stage3.1.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_23_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_23_bias:0"
    "ConstInputs": "backbone.stage3.1.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "445"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.1.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.1.branch2.7:0"
  top: "backbone.stage3.1.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_23_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_23_bias:0"
    "ConstInputs": "backbone.stage3.1.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-22-keras_quantization_wrapper_23"
  type: "Transform"
  bottom: "backbone.stage3.1.branch2.7:0"
  top: "transform-22-keras_quantization_wrapper_23:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "446"

  }

}
layer {
  name: "Concat: backbone.stage3.1/concat_1"
  type: "Concat"
  bottom: "transform-38-tf.__operators__.getitem_6:0"
  bottom: "transform-22-keras_quantization_wrapper_23:0"
  top: "backbone.stage3.1/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "448"

  }

}
layer {
  name: "Transform: transform-95-backbone.stage3.1/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.1/concat_1:0"
  top: "transform-95-backbone.stage3.1/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "449"

  }

}
layer {
  name: "Reshape: tf.reshape_10"
  type: "Reshape"
  bottom: "transform-95-backbone.stage3.1/concat_1:0"
  top: "tf.reshape_10:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "450"

  }

}
layer {
  name: "Transform: transform-83-tf.reshape_10"
  type: "Transform"
  bottom: "tf.reshape_10:0"
  top: "transform-83-tf.reshape_10:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "452"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_5"
  type: "Transpose"
  bottom: "transform-83-tf.reshape_10:0"
  top: "tf.compat.v1.transpose_5:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "453"

  }

}
layer {
  name: "Transform: transform-96-tf.compat.v1.transpose_5"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_5:0"
  top: "transform-96-tf.compat.v1.transpose_5:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "454"

  }

}
layer {
  name: "Reshape: tf.reshape_11"
  type: "Reshape"
  bottom: "transform-96-tf.compat.v1.transpose_5:0"
  top: "tf.reshape_11:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "455"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_8"
  type: "StrideSlice"
  bottom: "tf.reshape_11:0"
  top: "tf.__operators__.getitem_8:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "457"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_9"
  type: "StrideSlice"
  bottom: "tf.reshape_11:0"
  top: "tf.__operators__.getitem_9:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "458"

  }

}
layer {
  name: "Transform: transform-39-tf.__operators__.getitem_8"
  type: "Transform"
  bottom: "tf.__operators__.getitem_8:0"
  top: "transform-39-tf.__operators__.getitem_8:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "465"

  }

}
layer {
  name: "Transform: transform-21-tf.__operators__.getitem_9"
  type: "Transform"
  bottom: "tf.__operators__.getitem_9:0"
  top: "transform-21-tf.__operators__.getitem_9:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "459"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_24"
  type: "Convolution"
  bottom: "transform-21-tf.__operators__.getitem_9:0"
  top: "backbone.stage3.2.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_24_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_24_bias:0"
    "ConstInputs": "backbone.stage3.2.branch2.2_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "460"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.2.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.2.branch2.2:0"
  top: "backbone.stage3.2.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_24_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_24_bias:0"
    "ConstInputs": "backbone.stage3.2.branch2.2_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_25"
  type: "Convolution"
  bottom: "backbone.stage3.2.branch2.2:0"
  top: "keras_quantization_wrapper_25:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_25_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_25_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "461"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_26"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_25:0"
  top: "backbone.stage3.2.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_26_kernel:0"
    "ConstInputs": "backbone.stage3.2.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_26_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "463"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.2.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.2.branch2.7:0"
  top: "backbone.stage3.2.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_26_kernel:0"
    "ConstInputs": "backbone.stage3.2.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_26_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-20-keras_quantization_wrapper_26"
  type: "Transform"
  bottom: "backbone.stage3.2.branch2.7:0"
  top: "transform-20-keras_quantization_wrapper_26:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "464"

  }

}
layer {
  name: "Concat: backbone.stage3.2/concat_1"
  type: "Concat"
  bottom: "transform-39-tf.__operators__.getitem_8:0"
  bottom: "transform-20-keras_quantization_wrapper_26:0"
  top: "backbone.stage3.2/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "466"

  }

}
layer {
  name: "Transform: transform-97-backbone.stage3.2/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.2/concat_1:0"
  top: "transform-97-backbone.stage3.2/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "467"

  }

}
layer {
  name: "Reshape: tf.reshape_12"
  type: "Reshape"
  bottom: "transform-97-backbone.stage3.2/concat_1:0"
  top: "tf.reshape_12:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "468"

  }

}
layer {
  name: "Transform: transform-84-tf.reshape_12"
  type: "Transform"
  bottom: "tf.reshape_12:0"
  top: "transform-84-tf.reshape_12:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "470"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_6"
  type: "Transpose"
  bottom: "transform-84-tf.reshape_12:0"
  top: "tf.compat.v1.transpose_6:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "471"

  }

}
layer {
  name: "Transform: transform-98-tf.compat.v1.transpose_6"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_6:0"
  top: "transform-98-tf.compat.v1.transpose_6:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "472"

  }

}
layer {
  name: "Reshape: tf.reshape_13"
  type: "Reshape"
  bottom: "transform-98-tf.compat.v1.transpose_6:0"
  top: "tf.reshape_13:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "473"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_10"
  type: "StrideSlice"
  bottom: "tf.reshape_13:0"
  top: "tf.__operators__.getitem_10:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "475"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_11"
  type: "StrideSlice"
  bottom: "tf.reshape_13:0"
  top: "tf.__operators__.getitem_11:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "476"

  }

}
layer {
  name: "Transform: transform-40-tf.__operators__.getitem_10"
  type: "Transform"
  bottom: "tf.__operators__.getitem_10:0"
  top: "transform-40-tf.__operators__.getitem_10:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "483"

  }

}
layer {
  name: "Transform: transform-19-tf.__operators__.getitem_11"
  type: "Transform"
  bottom: "tf.__operators__.getitem_11:0"
  top: "transform-19-tf.__operators__.getitem_11:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "477"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_27"
  type: "Convolution"
  bottom: "transform-19-tf.__operators__.getitem_11:0"
  top: "backbone.stage3.3.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_27_kernel:0"
    "ConstInputs": "backbone.stage3.3.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_27_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "478"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.3.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.3.branch2.2:0"
  top: "backbone.stage3.3.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_27_kernel:0"
    "ConstInputs": "backbone.stage3.3.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_27_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_28"
  type: "Convolution"
  bottom: "backbone.stage3.3.branch2.2:0"
  top: "keras_quantization_wrapper_28:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_28_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_28_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "479"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_29"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_28:0"
  top: "backbone.stage3.3.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_29_kernel:0"
    "ConstInputs": "backbone.stage3.3.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_29_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "481"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.3.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.3.branch2.7:0"
  top: "backbone.stage3.3.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_29_kernel:0"
    "ConstInputs": "backbone.stage3.3.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_29_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-18-keras_quantization_wrapper_29"
  type: "Transform"
  bottom: "backbone.stage3.3.branch2.7:0"
  top: "transform-18-keras_quantization_wrapper_29:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "482"

  }

}
layer {
  name: "Concat: backbone.stage3.3/concat_1"
  type: "Concat"
  bottom: "transform-40-tf.__operators__.getitem_10:0"
  bottom: "transform-18-keras_quantization_wrapper_29:0"
  top: "backbone.stage3.3/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "484"

  }

}
layer {
  name: "Transform: transform-99-backbone.stage3.3/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.3/concat_1:0"
  top: "transform-99-backbone.stage3.3/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "485"

  }

}
layer {
  name: "Reshape: tf.reshape_14"
  type: "Reshape"
  bottom: "transform-99-backbone.stage3.3/concat_1:0"
  top: "tf.reshape_14:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "486"

  }

}
layer {
  name: "Transform: transform-85-tf.reshape_14"
  type: "Transform"
  bottom: "tf.reshape_14:0"
  top: "transform-85-tf.reshape_14:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "488"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_7"
  type: "Transpose"
  bottom: "transform-85-tf.reshape_14:0"
  top: "tf.compat.v1.transpose_7:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "489"

  }

}
layer {
  name: "Transform: transform-100-tf.compat.v1.transpose_7"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_7:0"
  top: "transform-100-tf.compat.v1.transpose_7:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "490"

  }

}
layer {
  name: "Reshape: tf.reshape_15"
  type: "Reshape"
  bottom: "transform-100-tf.compat.v1.transpose_7:0"
  top: "tf.reshape_15:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "491"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_12"
  type: "StrideSlice"
  bottom: "tf.reshape_15:0"
  top: "tf.__operators__.getitem_12:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "493"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_13"
  type: "StrideSlice"
  bottom: "tf.reshape_15:0"
  top: "tf.__operators__.getitem_13:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "494"

  }

}
layer {
  name: "Transform: transform-41-tf.__operators__.getitem_12"
  type: "Transform"
  bottom: "tf.__operators__.getitem_12:0"
  top: "transform-41-tf.__operators__.getitem_12:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "501"

  }

}
layer {
  name: "Transform: transform-17-tf.__operators__.getitem_13"
  type: "Transform"
  bottom: "tf.__operators__.getitem_13:0"
  top: "transform-17-tf.__operators__.getitem_13:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "495"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_30"
  type: "Convolution"
  bottom: "transform-17-tf.__operators__.getitem_13:0"
  top: "backbone.stage3.4.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_30_kernel:0"
    "ConstInputs": "backbone.stage3.4.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_30_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "496"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.4.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.4.branch2.2:0"
  top: "backbone.stage3.4.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_30_kernel:0"
    "ConstInputs": "backbone.stage3.4.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_30_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_31"
  type: "Convolution"
  bottom: "backbone.stage3.4.branch2.2:0"
  top: "keras_quantization_wrapper_31:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_31_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_31_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "497"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_32"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_31:0"
  top: "backbone.stage3.4.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_32_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_32_bias:0"
    "ConstInputs": "backbone.stage3.4.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "499"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.4.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.4.branch2.7:0"
  top: "backbone.stage3.4.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_32_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_32_bias:0"
    "ConstInputs": "backbone.stage3.4.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-16-keras_quantization_wrapper_32"
  type: "Transform"
  bottom: "backbone.stage3.4.branch2.7:0"
  top: "transform-16-keras_quantization_wrapper_32:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "500"

  }

}
layer {
  name: "Concat: backbone.stage3.4/concat_1"
  type: "Concat"
  bottom: "transform-41-tf.__operators__.getitem_12:0"
  bottom: "transform-16-keras_quantization_wrapper_32:0"
  top: "backbone.stage3.4/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "502"

  }

}
layer {
  name: "Transform: transform-101-backbone.stage3.4/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.4/concat_1:0"
  top: "transform-101-backbone.stage3.4/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "503"

  }

}
layer {
  name: "Reshape: tf.reshape_16"
  type: "Reshape"
  bottom: "transform-101-backbone.stage3.4/concat_1:0"
  top: "tf.reshape_16:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "504"

  }

}
layer {
  name: "Transform: transform-86-tf.reshape_16"
  type: "Transform"
  bottom: "tf.reshape_16:0"
  top: "transform-86-tf.reshape_16:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "506"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_8"
  type: "Transpose"
  bottom: "transform-86-tf.reshape_16:0"
  top: "tf.compat.v1.transpose_8:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "507"

  }

}
layer {
  name: "Transform: transform-102-tf.compat.v1.transpose_8"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_8:0"
  top: "transform-102-tf.compat.v1.transpose_8:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "508"

  }

}
layer {
  name: "Reshape: tf.reshape_17"
  type: "Reshape"
  bottom: "transform-102-tf.compat.v1.transpose_8:0"
  top: "tf.reshape_17:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "509"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_14"
  type: "StrideSlice"
  bottom: "tf.reshape_17:0"
  top: "tf.__operators__.getitem_14:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "511"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_15"
  type: "StrideSlice"
  bottom: "tf.reshape_17:0"
  top: "tf.__operators__.getitem_15:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "512"

  }

}
layer {
  name: "Transform: transform-42-tf.__operators__.getitem_14"
  type: "Transform"
  bottom: "tf.__operators__.getitem_14:0"
  top: "transform-42-tf.__operators__.getitem_14:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "519"

  }

}
layer {
  name: "Transform: transform-15-tf.__operators__.getitem_15"
  type: "Transform"
  bottom: "tf.__operators__.getitem_15:0"
  top: "transform-15-tf.__operators__.getitem_15:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "513"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_33"
  type: "Convolution"
  bottom: "transform-15-tf.__operators__.getitem_15:0"
  top: "backbone.stage3.5.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_33_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_33_bias:0"
    "ConstInputs": "backbone.stage3.5.branch2.2_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "514"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.5.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.5.branch2.2:0"
  top: "backbone.stage3.5.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_33_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_33_bias:0"
    "ConstInputs": "backbone.stage3.5.branch2.2_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_34"
  type: "Convolution"
  bottom: "backbone.stage3.5.branch2.2:0"
  top: "keras_quantization_wrapper_34:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_34_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_34_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "515"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_35"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_34:0"
  top: "backbone.stage3.5.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_35_kernel:0"
    "ConstInputs": "backbone.stage3.5.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_35_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "517"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.5.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.5.branch2.7:0"
  top: "backbone.stage3.5.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_35_kernel:0"
    "ConstInputs": "backbone.stage3.5.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_35_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-14-keras_quantization_wrapper_35"
  type: "Transform"
  bottom: "backbone.stage3.5.branch2.7:0"
  top: "transform-14-keras_quantization_wrapper_35:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "518"

  }

}
layer {
  name: "Concat: backbone.stage3.5/concat_1"
  type: "Concat"
  bottom: "transform-14-keras_quantization_wrapper_35:0"
  bottom: "transform-42-tf.__operators__.getitem_14:0"
  top: "backbone.stage3.5/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "520"

  }

}
layer {
  name: "Transform: transform-103-backbone.stage3.5/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.5/concat_1:0"
  top: "transform-103-backbone.stage3.5/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "521"

  }

}
layer {
  name: "Reshape: tf.reshape_18"
  type: "Reshape"
  bottom: "transform-103-backbone.stage3.5/concat_1:0"
  top: "tf.reshape_18:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "522"

  }

}
layer {
  name: "Transform: transform-87-tf.reshape_18"
  type: "Transform"
  bottom: "tf.reshape_18:0"
  top: "transform-87-tf.reshape_18:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "524"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_9"
  type: "Transpose"
  bottom: "transform-87-tf.reshape_18:0"
  top: "tf.compat.v1.transpose_9:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "525"

  }

}
layer {
  name: "Transform: transform-104-tf.compat.v1.transpose_9"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_9:0"
  top: "transform-104-tf.compat.v1.transpose_9:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "526"

  }

}
layer {
  name: "Reshape: tf.reshape_19"
  type: "Reshape"
  bottom: "transform-104-tf.compat.v1.transpose_9:0"
  top: "tf.reshape_19:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "527"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_16"
  type: "StrideSlice"
  bottom: "tf.reshape_19:0"
  top: "tf.__operators__.getitem_16:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "529"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_17"
  type: "StrideSlice"
  bottom: "tf.reshape_19:0"
  top: "tf.__operators__.getitem_17:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "530"

  }

}
layer {
  name: "Transform: transform-43-tf.__operators__.getitem_16"
  type: "Transform"
  bottom: "tf.__operators__.getitem_16:0"
  top: "transform-43-tf.__operators__.getitem_16:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "537"

  }

}
layer {
  name: "Transform: transform-13-tf.__operators__.getitem_17"
  type: "Transform"
  bottom: "tf.__operators__.getitem_17:0"
  top: "transform-13-tf.__operators__.getitem_17:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "531"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_36"
  type: "Convolution"
  bottom: "transform-13-tf.__operators__.getitem_17:0"
  top: "backbone.stage3.6.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_36_kernel:0"
    "ConstInputs": "backbone.stage3.6.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_36_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "532"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.6.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.6.branch2.2:0"
  top: "backbone.stage3.6.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_36_kernel:0"
    "ConstInputs": "backbone.stage3.6.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_36_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_37"
  type: "Convolution"
  bottom: "backbone.stage3.6.branch2.2:0"
  top: "keras_quantization_wrapper_37:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_37_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_37_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "533"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_38"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_37:0"
  top: "backbone.stage3.6.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_38_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_38_bias:0"
    "ConstInputs": "backbone.stage3.6.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "535"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.6.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.6.branch2.7:0"
  top: "backbone.stage3.6.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_38_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_38_bias:0"
    "ConstInputs": "backbone.stage3.6.branch2.7_slope:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-12-keras_quantization_wrapper_38"
  type: "Transform"
  bottom: "backbone.stage3.6.branch2.7:0"
  top: "transform-12-keras_quantization_wrapper_38:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "536"

  }

}
layer {
  name: "Concat: backbone.stage3.6/concat_1"
  type: "Concat"
  bottom: "transform-43-tf.__operators__.getitem_16:0"
  bottom: "transform-12-keras_quantization_wrapper_38:0"
  top: "backbone.stage3.6/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "538"

  }

}
layer {
  name: "Transform: transform-106-backbone.stage3.6/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.6/concat_1:0"
  top: "transform-106-backbone.stage3.6/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "539"

  }

}
layer {
  name: "Reshape: tf.reshape_20"
  type: "Reshape"
  bottom: "transform-106-backbone.stage3.6/concat_1:0"
  top: "tf.reshape_20:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "540"

  }

}
layer {
  name: "Transform: transform-73-tf.reshape_20"
  type: "Transform"
  bottom: "tf.reshape_20:0"
  top: "transform-73-tf.reshape_20:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "542"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_10"
  type: "Transpose"
  bottom: "transform-73-tf.reshape_20:0"
  top: "tf.compat.v1.transpose_10:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "543"

  }

}
layer {
  name: "Transform: transform-107-tf.compat.v1.transpose_10"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_10:0"
  top: "transform-107-tf.compat.v1.transpose_10:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "544"

  }

}
layer {
  name: "Reshape: tf.reshape_21"
  type: "Reshape"
  bottom: "transform-107-tf.compat.v1.transpose_10:0"
  top: "tf.reshape_21:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "545"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_18"
  type: "StrideSlice"
  bottom: "tf.reshape_21:0"
  top: "tf.__operators__.getitem_18:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "547"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_19"
  type: "StrideSlice"
  bottom: "tf.reshape_21:0"
  top: "tf.__operators__.getitem_19:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "548"

  }

}
layer {
  name: "Transform: transform-44-tf.__operators__.getitem_18"
  type: "Transform"
  bottom: "tf.__operators__.getitem_18:0"
  top: "transform-44-tf.__operators__.getitem_18:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "555"

  }

}
layer {
  name: "Transform: transform-11-tf.__operators__.getitem_19"
  type: "Transform"
  bottom: "tf.__operators__.getitem_19:0"
  top: "transform-11-tf.__operators__.getitem_19:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "549"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_39"
  type: "Convolution"
  bottom: "transform-11-tf.__operators__.getitem_19:0"
  top: "backbone.stage3.7.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_39_kernel:0"
    "ConstInputs": "backbone.stage3.7.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_39_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "550"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.7.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage3.7.branch2.2:0"
  top: "backbone.stage3.7.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_39_kernel:0"
    "ConstInputs": "backbone.stage3.7.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_39_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_40"
  type: "Convolution"
  bottom: "backbone.stage3.7.branch2.2:0"
  top: "keras_quantization_wrapper_40:0"
    scale_param {
    "InputShapes": "26x26x176, 3x3x1x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_40_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_40_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "551"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_41"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_40:0"
  top: "backbone.stage3.7.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1x1x176x176, 176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_41_kernel:0"
    "ConstInputs": "backbone.stage3.7.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_41_bias:0"
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "553"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 176
      dim: 176
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 176
    }
  }
}
layer {
  name: "Prelu: backbone.stage3.7.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage3.7.branch2.7:0"
  top: "backbone.stage3.7.branch2.7:0"
    scale_param {
    "InputShapes": "26x26x176, 1"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_41_kernel:0"
    "ConstInputs": "backbone.stage3.7.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_41_bias:0"
    "RuntimeMemory": "126.75KB"
    
  }

}
layer {
  name: "Transform: transform-10-keras_quantization_wrapper_41"
  type: "Transform"
  bottom: "backbone.stage3.7.branch2.7:0"
  top: "transform-10-keras_quantization_wrapper_41:0"
    scale_param {
    "InputShapes": "26x26x176"
    "OutputShape": "26x26x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "126.75KB"
    "ScheduleId": "554"

  }

}
layer {
  name: "Concat: backbone.stage3.7/concat_1"
  type: "Concat"
  bottom: "transform-44-tf.__operators__.getitem_18:0"
  bottom: "transform-10-keras_quantization_wrapper_41:0"
  top: "backbone.stage3.7/concat_1:0"
    scale_param {
    "InputShapes": "26x26x176, 26x26x176"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "556"

  }

}
layer {
  name: "Transform: transform-108-backbone.stage3.7/concat_1"
  type: "Transform"
  bottom: "backbone.stage3.7/concat_1:0"
  top: "transform-108-backbone.stage3.7/concat_1:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "557"

  }

}
layer {
  name: "Reshape: tf.reshape_22"
  type: "Reshape"
  bottom: "transform-108-backbone.stage3.7/concat_1:0"
  top: "tf.reshape_22:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "558"

  }

}
layer {
  name: "Transform: transform-74-tf.reshape_22"
  type: "Transform"
  bottom: "tf.reshape_22:0"
  top: "transform-74-tf.reshape_22:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x2x176"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "560"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_11"
  type: "Transpose"
  bottom: "transform-74-tf.reshape_22:0"
  top: "tf.compat.v1.transpose_11:0"
    scale_param {
    "InputShapes": "26x26x2x176"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "561"

  }

}
layer {
  name: "Transform: transform-109-tf.compat.v1.transpose_11"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_11:0"
  top: "transform-109-tf.compat.v1.transpose_11:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x176x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "562"

  }

}
layer {
  name: "Reshape: tf.reshape_23"
  type: "Reshape"
  bottom: "transform-109-tf.compat.v1.transpose_11:0"
  top: "tf.reshape_23:0"
    scale_param {
    "InputShapes": "26x26x176x2"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "563"

  }

}
layer {
  name: "Transform: transform-1-tf.reshape_23"
  type: "Transform"
  bottom: "tf.reshape_23:0"
  top: "transform-1-tf.reshape_23:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "565"

  }

}
layer {
  name: "Transform: transform-9-tf.reshape_23"
  type: "Transform"
  bottom: "tf.reshape_23:0"
  top: "transform-9-tf.reshape_23:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "571"

  }

}
layer {
  name: "Transform: transform-56-tf.reshape_23"
  type: "Transform"
  bottom: "tf.reshape_23:0"
  top: "transform-56-tf.reshape_23:0"
    scale_param {
    "InputShapes": "26x26x352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "567"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_42"
  type: "Convolution"
  bottom: "transform-1-tf.reshape_23:0"
  top: "leaky_re_lu_2:0"
    scale_param {
    "InputShapes": "26x26x352, 1x1x352x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_42_kernel:0"
    "ConstInputs": "leaky_re_lu_2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_42_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "566"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_2"
  type: "Prelu"
  bottom: "leaky_re_lu_2:0"
  top: "leaky_re_lu_2:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_42_kernel:0"
    "ConstInputs": "leaky_re_lu_2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_42_bias:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_43"
  type: "Convolution"
  bottom: "transform-9-tf.reshape_23:0"
  top: "backbone.stage4.0.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x352, 1x1x352x352, 352"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_43_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_43_bias:0"
    "ConstInputs": "backbone.stage4.0.branch2.2_slope:0"
    "RuntimeMemory": "232.38KB"
    "ScheduleId": "572"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.0.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage4.0.branch2.2:0"
  top: "backbone.stage4.0.branch2.2:0"
    scale_param {
    "InputShapes": "26x26x352, 1"
    "OutputShape": "26x26x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_43_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_43_bias:0"
    "ConstInputs": "backbone.stage4.0.branch2.2_slope:0"
    "RuntimeMemory": "232.38KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_44"
  type: "Convolution"
  bottom: "transform-56-tf.reshape_23:0"
  top: "keras_quantization_wrapper_44:0"
    scale_param {
    "InputShapes": "26x26x352, 3x3x1x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_44_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_44_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "568"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_46"
  type: "Convolution"
  bottom: "backbone.stage4.0.branch2.2:0"
  top: "keras_quantization_wrapper_46:0"
    scale_param {
    "InputShapes": "26x26x352, 3x3x1x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_46_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_46_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "573"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_45"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_44:0"
  top: "backbone.stage4.0.branch1.4:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_45_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_45_bias:0"
    "ConstInputs": "backbone.stage4.0.branch1.4_slope:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "570"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.0.branch1.4"
  type: "Prelu"
  bottom: "backbone.stage4.0.branch1.4:0"
  top: "backbone.stage4.0.branch1.4:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_45_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_45_bias:0"
    "ConstInputs": "backbone.stage4.0.branch1.4_slope:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_47"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_46:0"
  top: "backbone.stage4.0.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_47_kernel:0"
    "ConstInputs": "backbone.stage4.0.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_47_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "575"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.0.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage4.0.branch2.7:0"
  top: "backbone.stage4.0.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_47_kernel:0"
    "ConstInputs": "backbone.stage4.0.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_47_bias:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Concat: backbone.stage4.0/concat_2"
  type: "Concat"
  bottom: "backbone.stage4.0.branch1.4:0"
  bottom: "backbone.stage4.0.branch2.7:0"
  top: "backbone.stage4.0/concat_2:0"
    scale_param {
    "InputShapes": "13x13x352, 13x13x352"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "116.19KB"
    "ScheduleId": "576"

  }

}
layer {
  name: "Transform: transform-110-backbone.stage4.0/concat_2"
  type: "Transform"
  bottom: "backbone.stage4.0/concat_2:0"
  top: "transform-110-backbone.stage4.0/concat_2:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "577"

  }

}
layer {
  name: "Reshape: tf.reshape_24"
  type: "Reshape"
  bottom: "transform-110-backbone.stage4.0/concat_2:0"
  top: "tf.reshape_24:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "578"

  }

}
layer {
  name: "Transform: transform-75-tf.reshape_24"
  type: "Transform"
  bottom: "tf.reshape_24:0"
  top: "transform-75-tf.reshape_24:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "580"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_12"
  type: "Transpose"
  bottom: "transform-75-tf.reshape_24:0"
  top: "tf.compat.v1.transpose_12:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "581"

  }

}
layer {
  name: "Transform: transform-111-tf.compat.v1.transpose_12"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_12:0"
  top: "transform-111-tf.compat.v1.transpose_12:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "582"

  }

}
layer {
  name: "Reshape: tf.reshape_25"
  type: "Reshape"
  bottom: "transform-111-tf.compat.v1.transpose_12:0"
  top: "tf.reshape_25:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "583"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_20"
  type: "StrideSlice"
  bottom: "tf.reshape_25:0"
  top: "tf.__operators__.getitem_20:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "585"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_21"
  type: "StrideSlice"
  bottom: "tf.reshape_25:0"
  top: "tf.__operators__.getitem_21:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "587"

  }

}
layer {
  name: "Cast: backbone.stage4.1/concat_1_qtype_pre_cast_0"
  type: "Cast"
  bottom: "tf.__operators__.getitem_20:0"
  top: "backbone.stage4.1/concat_1_qtype_pre_cast_0:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "586"

  }

}
layer {
  name: "Transform: transform-8-tf.__operators__.getitem_21"
  type: "Transform"
  bottom: "tf.__operators__.getitem_21:0"
  top: "transform-8-tf.__operators__.getitem_21:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "589"

  }

}
layer {
  name: "Transform: transform-45-backbone.stage4.1/concat_1_qtype_pre_cast_0"
  type: "Transform"
  bottom: "backbone.stage4.1/concat_1_qtype_pre_cast_0:0"
  top: "transform-45-backbone.stage4.1/concat_1_qtype_pre_cast_0:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "588"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_48"
  type: "Convolution"
  bottom: "transform-8-tf.__operators__.getitem_21:0"
  top: "backbone.stage4.1.branch2.2:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_48_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_48_bias:0"
    "ConstInputs": "backbone.stage4.1.branch2.2_slope:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "590"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.1.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage4.1.branch2.2:0"
  top: "backbone.stage4.1.branch2.2:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_48_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_48_bias:0"
    "ConstInputs": "backbone.stage4.1.branch2.2_slope:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_49"
  type: "Convolution"
  bottom: "backbone.stage4.1.branch2.2:0"
  top: "keras_quantization_wrapper_49:0"
    scale_param {
    "InputShapes": "13x13x352, 3x3x1x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_49_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_49_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "591"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_50"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_49:0"
  top: "backbone.stage4.1.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_50_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_50_bias:0"
    "ConstInputs": "backbone.stage4.1.branch2.7_slope:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "593"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.1.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage4.1.branch2.7:0"
  top: "backbone.stage4.1.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_50_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_50_bias:0"
    "ConstInputs": "backbone.stage4.1.branch2.7_slope:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Transform: transform-7-keras_quantization_wrapper_50"
  type: "Transform"
  bottom: "backbone.stage4.1.branch2.7:0"
  top: "transform-7-keras_quantization_wrapper_50:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "594"

  }

}
layer {
  name: "Concat: backbone.stage4.1/concat_1"
  type: "Concat"
  bottom: "transform-45-backbone.stage4.1/concat_1_qtype_pre_cast_0:0"
  bottom: "transform-7-keras_quantization_wrapper_50:0"
  top: "backbone.stage4.1/concat_1:0"
    scale_param {
    "InputShapes": "13x13x352, 13x13x352"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "116.19KB"
    "ScheduleId": "595"

  }

}
layer {
  name: "Transform: transform-112-backbone.stage4.1/concat_1"
  type: "Transform"
  bottom: "backbone.stage4.1/concat_1:0"
  top: "transform-112-backbone.stage4.1/concat_1:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "596"

  }

}
layer {
  name: "Reshape: tf.reshape_26"
  type: "Reshape"
  bottom: "transform-112-backbone.stage4.1/concat_1:0"
  top: "tf.reshape_26:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "597"

  }

}
layer {
  name: "Transform: transform-76-tf.reshape_26"
  type: "Transform"
  bottom: "tf.reshape_26:0"
  top: "transform-76-tf.reshape_26:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "599"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_13"
  type: "Transpose"
  bottom: "transform-76-tf.reshape_26:0"
  top: "tf.compat.v1.transpose_13:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "600"

  }

}
layer {
  name: "Transform: transform-113-tf.compat.v1.transpose_13"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_13:0"
  top: "transform-113-tf.compat.v1.transpose_13:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "601"

  }

}
layer {
  name: "Reshape: tf.reshape_27"
  type: "Reshape"
  bottom: "transform-113-tf.compat.v1.transpose_13:0"
  top: "tf.reshape_27:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "602"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_22"
  type: "StrideSlice"
  bottom: "tf.reshape_27:0"
  top: "tf.__operators__.getitem_22:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "604"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_23"
  type: "StrideSlice"
  bottom: "tf.reshape_27:0"
  top: "tf.__operators__.getitem_23:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "605"

  }

}
layer {
  name: "Transform: transform-46-tf.__operators__.getitem_22"
  type: "Transform"
  bottom: "tf.__operators__.getitem_22:0"
  top: "transform-46-tf.__operators__.getitem_22:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "606"

  }

}
layer {
  name: "Transform: transform-6-tf.__operators__.getitem_23"
  type: "Transform"
  bottom: "tf.__operators__.getitem_23:0"
  top: "transform-6-tf.__operators__.getitem_23:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "607"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_51"
  type: "Convolution"
  bottom: "transform-6-tf.__operators__.getitem_23:0"
  top: "backbone.stage4.2.branch2.2:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_51_kernel:0"
    "ConstInputs": "backbone.stage4.2.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_51_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "608"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.2.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage4.2.branch2.2:0"
  top: "backbone.stage4.2.branch2.2:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_51_kernel:0"
    "ConstInputs": "backbone.stage4.2.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_51_bias:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_52"
  type: "Convolution"
  bottom: "backbone.stage4.2.branch2.2:0"
  top: "keras_quantization_wrapper_52:0"
    scale_param {
    "InputShapes": "13x13x352, 3x3x1x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_52_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_52_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "609"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_53"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_52:0"
  top: "backbone.stage4.2.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_53_kernel:0"
    "ConstInputs": "backbone.stage4.2.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_53_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "611"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.2.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage4.2.branch2.7:0"
  top: "backbone.stage4.2.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_53_kernel:0"
    "ConstInputs": "backbone.stage4.2.branch2.7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_53_bias:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Cast: backbone.stage4.2/concat_1_qtype_pre_cast_0"
  type: "Cast"
  bottom: "backbone.stage4.2.branch2.7:0"
  top: "backbone.stage4.2/concat_1_qtype_pre_cast_0:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "612"

  }

}
layer {
  name: "Transform: transform-47-backbone.stage4.2/concat_1_qtype_pre_cast_0"
  type: "Transform"
  bottom: "backbone.stage4.2/concat_1_qtype_pre_cast_0:0"
  top: "transform-47-backbone.stage4.2/concat_1_qtype_pre_cast_0:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "613"

  }

}
layer {
  name: "Concat: backbone.stage4.2/concat_1"
  type: "Concat"
  bottom: "transform-46-tf.__operators__.getitem_22:0"
  bottom: "transform-47-backbone.stage4.2/concat_1_qtype_pre_cast_0:0"
  top: "backbone.stage4.2/concat_1:0"
    scale_param {
    "InputShapes": "13x13x352, 13x13x352"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "116.19KB"
    "ScheduleId": "614"

  }

}
layer {
  name: "Transform: transform-114-backbone.stage4.2/concat_1"
  type: "Transform"
  bottom: "backbone.stage4.2/concat_1:0"
  top: "transform-114-backbone.stage4.2/concat_1:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "615"

  }

}
layer {
  name: "Reshape: tf.reshape_28"
  type: "Reshape"
  bottom: "transform-114-backbone.stage4.2/concat_1:0"
  top: "tf.reshape_28:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "616"

  }

}
layer {
  name: "Transform: transform-77-tf.reshape_28"
  type: "Transform"
  bottom: "tf.reshape_28:0"
  top: "transform-77-tf.reshape_28:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "618"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_14"
  type: "Transpose"
  bottom: "transform-77-tf.reshape_28:0"
  top: "tf.compat.v1.transpose_14:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "619"

  }

}
layer {
  name: "Transform: transform-115-tf.compat.v1.transpose_14"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_14:0"
  top: "transform-115-tf.compat.v1.transpose_14:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "620"

  }

}
layer {
  name: "Reshape: tf.reshape_29"
  type: "Reshape"
  bottom: "transform-115-tf.compat.v1.transpose_14:0"
  top: "tf.reshape_29:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "621"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_24"
  type: "StrideSlice"
  bottom: "tf.reshape_29:0"
  top: "tf.__operators__.getitem_24:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "623"

  }

}
layer {
  name: "StrideSlice: tf.__operators__.getitem_25"
  type: "StrideSlice"
  bottom: "tf.reshape_29:0"
  top: "tf.__operators__.getitem_25:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "143.00KB"
    "ScheduleId": "624"

  }

}
layer {
  name: "Transform: transform-48-tf.__operators__.getitem_24"
  type: "Transform"
  bottom: "tf.__operators__.getitem_24:0"
  top: "transform-48-tf.__operators__.getitem_24:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "625"

  }

}
layer {
  name: "Transform: transform-5-tf.__operators__.getitem_25"
  type: "Transform"
  bottom: "tf.__operators__.getitem_25:0"
  top: "transform-5-tf.__operators__.getitem_25:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "626"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_54"
  type: "Convolution"
  bottom: "transform-5-tf.__operators__.getitem_25:0"
  top: "backbone.stage4.3.branch2.2:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_54_kernel:0"
    "ConstInputs": "backbone.stage4.3.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_54_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "627"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.3.branch2.2"
  type: "Prelu"
  bottom: "backbone.stage4.3.branch2.2:0"
  top: "backbone.stage4.3.branch2.2:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_54_kernel:0"
    "ConstInputs": "backbone.stage4.3.branch2.2_slope:0"
    "ConstInputs": "keras_quantization_wrapper_54_bias:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_55"
  type: "Convolution"
  bottom: "backbone.stage4.3.branch2.2:0"
  top: "keras_quantization_wrapper_55:0"
    scale_param {
    "InputShapes": "13x13x352, 3x3x1x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-32.0, 32.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_55_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_55_bias:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "628"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_56"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_55:0"
  top: "backbone.stage4.3.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1x1x352x352, 352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_56_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_56_bias:0"
    "ConstInputs": "backbone.stage4.3.branch2.7_slope:0"
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "630"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 352
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 352
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: backbone.stage4.3.branch2.7"
  type: "Prelu"
  bottom: "backbone.stage4.3.branch2.7:0"
  top: "backbone.stage4.3.branch2.7:0"
    scale_param {
    "InputShapes": "13x13x352, 1"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_56_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_56_bias:0"
    "ConstInputs": "backbone.stage4.3.branch2.7_slope:0"
    "RuntimeMemory": "58.09KB"
    
  }

}
layer {
  name: "Transform: transform-4-keras_quantization_wrapper_56"
  type: "Transform"
  bottom: "backbone.stage4.3.branch2.7:0"
  top: "transform-4-keras_quantization_wrapper_56:0"
    scale_param {
    "InputShapes": "13x13x352"
    "OutputShape": "13x13x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "58.09KB"
    "ScheduleId": "631"

  }

}
layer {
  name: "Concat: backbone.stage4.3/concat_1"
  type: "Concat"
  bottom: "transform-48-tf.__operators__.getitem_24:0"
  bottom: "transform-4-keras_quantization_wrapper_56:0"
  top: "backbone.stage4.3/concat_1:0"
    scale_param {
    "InputShapes": "13x13x352, 13x13x352"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "116.19KB"
    "ScheduleId": "632"

  }

}
layer {
  name: "Transform: transform-117-backbone.stage4.3/concat_1"
  type: "Transform"
  bottom: "backbone.stage4.3/concat_1:0"
  top: "transform-117-backbone.stage4.3/concat_1:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "633"

  }

}
layer {
  name: "Reshape: tf.reshape_30"
  type: "Reshape"
  bottom: "transform-117-backbone.stage4.3/concat_1:0"
  top: "tf.reshape_30:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "634"

  }

}
layer {
  name: "Transform: transform-78-tf.reshape_30"
  type: "Transform"
  bottom: "tf.reshape_30:0"
  top: "transform-78-tf.reshape_30:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x2x352"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "636"

  }

}
layer {
  name: "Transpose: tf.compat.v1.transpose_15"
  type: "Transpose"
  bottom: "transform-78-tf.reshape_30:0"
  top: "tf.compat.v1.transpose_15:0"
    scale_param {
    "InputShapes": "13x13x2x352"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "637"

  }

}
layer {
  name: "Transform: transform-118-tf.compat.v1.transpose_15"
  type: "Transform"
  bottom: "tf.compat.v1.transpose_15:0"
  top: "transform-118-tf.compat.v1.transpose_15:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x352x2"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "638"

  }

}
layer {
  name: "Reshape: tf.reshape_31"
  type: "Reshape"
  bottom: "transform-118-tf.compat.v1.transpose_15:0"
  top: "tf.reshape_31:0"
    scale_param {
    "InputShapes": "13x13x352x2"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "286.00KB"
    "ScheduleId": "639"

  }

}
layer {
  name: "Transform: transform-0-tf.reshape_31"
  type: "Transform"
  bottom: "tf.reshape_31:0"
  top: "transform-0-tf.reshape_31:0"
    scale_param {
    "InputShapes": "13x13x704"
    "OutputShape": "13x13x704"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "116.19KB"
    "ScheduleId": "641"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_57"
  type: "Convolution"
  bottom: "transform-0-tf.reshape_31:0"
  top: "leaky_re_lu_3:0"
    scale_param {
    "InputShapes": "13x13x704, 1x1x704x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_57_kernel:0"
    "ConstInputs": "leaky_re_lu_3_slope:0"
    "ConstInputs": "keras_quantization_wrapper_57_bias:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "642"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 704
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_3"
  type: "Prelu"
  bottom: "leaky_re_lu_3:0"
  top: "leaky_re_lu_3:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_57_kernel:0"
    "ConstInputs": "leaky_re_lu_3_slope:0"
    "ConstInputs": "keras_quantization_wrapper_57_bias:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_58"
  type: "Convolution"
  bottom: "leaky_re_lu_3:0"
  top: "leaky_re_lu_18:0"
    scale_param {
    "InputShapes": "13x13x128, 5x5x1x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_58_kernel:0"
    "ConstInputs": "leaky_re_lu_18_slope:0"
    "ConstInputs": "keras_quantization_wrapper_58_bias:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "643"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_18"
  type: "Prelu"
  bottom: "leaky_re_lu_18:0"
  top: "leaky_re_lu_18:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_58_kernel:0"
    "ConstInputs": "leaky_re_lu_18_slope:0"
    "ConstInputs": "keras_quantization_wrapper_58_bias:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Resize: resizing"
  type: "Resize"
  bottom: "leaky_re_lu_3:0"
  top: "resizing:0"
    scale_param {
    "InputShapes": "13x13x128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "646"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_61"
  type: "Convolution"
  bottom: "leaky_re_lu_18:0"
  top: "leaky_re_lu_19:0"
    scale_param {
    "InputShapes": "7x7x128, 1x1x128x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_61_kernel:0"
    "ConstInputs": "leaky_re_lu_19_slope:0"
    "ConstInputs": "keras_quantization_wrapper_61_bias:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "645"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_19"
  type: "Prelu"
  bottom: "leaky_re_lu_19:0"
  top: "leaky_re_lu_19:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_61_kernel:0"
    "ConstInputs": "leaky_re_lu_19_slope:0"
    "ConstInputs": "keras_quantization_wrapper_61_bias:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Concat: p3_input"
  type: "Concat"
  bottom: "resizing:0"
  bottom: "leaky_re_lu_2:0"
  top: "p3_input:0"
    scale_param {
    "InputShapes": "26x26x128, 26x26x128"
    "OutputShape": "26x26x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "648"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_59"
  type: "Convolution"
  bottom: "p3_input:0"
  top: "keras_quantization_wrapper_59:0"
    scale_param {
    "InputShapes": "26x26x256, 5x5x1x256, 256"
    "OutputShape": "26x26x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_59_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_59_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "649"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 256
    }
  }
  blobs {
    shape {
      dim: 256
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_60"
  type: "Convolution"
  bottom: "p3_input:0"
  top: "leaky_re_lu_4:0"
    scale_param {
    "InputShapes": "26x26x256, 1x1x256x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_60_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_60_bias:0"
    "ConstInputs": "leaky_re_lu_4_slope:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "651"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_4"
  type: "Prelu"
  bottom: "leaky_re_lu_4:0"
  top: "leaky_re_lu_4:0"
    scale_param {
    "InputShapes": "26x26x64, 1"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_60_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_60_bias:0"
    "ConstInputs": "leaky_re_lu_4_slope:0"
    "RuntimeMemory": "42.25KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_63"
  type: "Convolution"
  bottom: "leaky_re_lu_4:0"
  top: "leaky_re_lu_5:0"
    scale_param {
    "InputShapes": "26x26x64, 3x3x1x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_63_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_63_bias:0"
    "ConstInputs": "leaky_re_lu_5_slope:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "652"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_5"
  type: "Prelu"
  bottom: "leaky_re_lu_5:0"
  top: "leaky_re_lu_5:0"
    scale_param {
    "InputShapes": "26x26x64, 1"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_63_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_63_bias:0"
    "ConstInputs": "leaky_re_lu_5_slope:0"
    "RuntimeMemory": "42.25KB"
    
  }

}
layer {
  name: "Concat: concatenate"
  type: "Concat"
  bottom: "leaky_re_lu_4:0"
  bottom: "leaky_re_lu_5:0"
  top: "concatenate:0"
    scale_param {
    "InputShapes": "26x26x64, 26x26x64"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "654"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_64"
  type: "Convolution"
  bottom: "concatenate:0"
  top: "keras_quantization_wrapper_64:0"
    scale_param {
    "InputShapes": "26x26x128, 1x1x128x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_64_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_64_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "655"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_65"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_64:0"
  top: "keras_quantization_wrapper_65:0"
    scale_param {
    "InputShapes": "26x26x64, 3x3x1x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_65_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_65_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "656"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Concat: concatenate_1"
  type: "Concat"
  bottom: "keras_quantization_wrapper_65:0"
  bottom: "keras_quantization_wrapper_64:0"
  top: "concatenate_1:0"
    scale_param {
    "InputShapes": "26x26x64, 26x26x64"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "658"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_62"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_59:0"
  bottom: "concatenate_1:0"
  top: "add:0"
    scale_param {
    "InputShapes": "26x26x256, 1x1x256x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_62_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_62_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "659"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Add: add"
  type: "Add"
  bottom: "add:0"
  top: "add:0"
    scale_param {
    "InputShapes": "26x26x128, 26x26x128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_62_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_62_bias:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Resize: resizing_1"
  type: "Resize"
  bottom: "add:0"
  top: "resizing_1:0"
    scale_param {
    "InputShapes": "26x26x128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "660"

  }

}
layer {
  name: "Concat: p2_input"
  type: "Concat"
  bottom: "leaky_re_lu_1:0"
  bottom: "resizing_1:0"
  top: "p2_input:0"
    scale_param {
    "InputShapes": "52x52x128, 52x52x128"
    "OutputShape": "52x52x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "676.00KB"
    "ScheduleId": "662"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_66"
  type: "Convolution"
  bottom: "p2_input:0"
  top: "keras_quantization_wrapper_66:0"
    scale_param {
    "InputShapes": "52x52x256, 5x5x1x256, 256"
    "OutputShape": "52x52x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_66_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_66_bias:0"
    "RuntimeMemory": "676.00KB"
    "ScheduleId": "663"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 256
    }
  }
  blobs {
    shape {
      dim: 256
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_67"
  type: "Convolution"
  bottom: "p2_input:0"
  top: "leaky_re_lu_6:0"
    scale_param {
    "InputShapes": "52x52x256, 1x1x256x64, 64"
    "OutputShape": "52x52x64"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_67_kernel:0"
    "ConstInputs": "leaky_re_lu_6_slope:0"
    "ConstInputs": "keras_quantization_wrapper_67_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "665"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_6"
  type: "Prelu"
  bottom: "leaky_re_lu_6:0"
  top: "leaky_re_lu_6:0"
    scale_param {
    "InputShapes": "52x52x64, 1"
    "OutputShape": "52x52x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_67_kernel:0"
    "ConstInputs": "leaky_re_lu_6_slope:0"
    "ConstInputs": "keras_quantization_wrapper_67_bias:0"
    "RuntimeMemory": "169.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_69"
  type: "Convolution"
  bottom: "leaky_re_lu_6:0"
  top: "leaky_re_lu_7:0"
    scale_param {
    "InputShapes": "52x52x64, 3x3x1x64, 64"
    "OutputShape": "52x52x64"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_69_kernel:0"
    "ConstInputs": "leaky_re_lu_7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_69_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "666"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_7"
  type: "Prelu"
  bottom: "leaky_re_lu_7:0"
  top: "leaky_re_lu_7:0"
    scale_param {
    "InputShapes": "52x52x64, 1"
    "OutputShape": "52x52x64"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_69_kernel:0"
    "ConstInputs": "leaky_re_lu_7_slope:0"
    "ConstInputs": "keras_quantization_wrapper_69_bias:0"
    "RuntimeMemory": "169.00KB"
    
  }

}
layer {
  name: "Concat: concatenate_2"
  type: "Concat"
  bottom: "leaky_re_lu_7:0"
  bottom: "leaky_re_lu_6:0"
  top: "concatenate_2:0"
    scale_param {
    "InputShapes": "52x52x64, 52x52x64"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "668"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_70"
  type: "Convolution"
  bottom: "concatenate_2:0"
  top: "keras_quantization_wrapper_70:0"
    scale_param {
    "InputShapes": "52x52x128, 1x1x128x64, 64"
    "OutputShape": "52x52x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_70_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_70_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "669"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_71"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_70:0"
  top: "keras_quantization_wrapper_71:0"
    scale_param {
    "InputShapes": "52x52x64, 3x3x1x64, 64"
    "OutputShape": "52x52x64"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_71_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_71_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "670"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Concat: concatenate_3"
  type: "Concat"
  bottom: "keras_quantization_wrapper_71:0"
  bottom: "keras_quantization_wrapper_70:0"
  top: "concatenate_3:0"
    scale_param {
    "InputShapes": "52x52x64, 52x52x64"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "672"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_68"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_66:0"
  bottom: "concatenate_3:0"
  top: "add_1:0"
    scale_param {
    "InputShapes": "52x52x256, 1x1x256x128, 128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_68_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_68_bias:0"
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "673"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Add: add_1"
  type: "Add"
  bottom: "add_1:0"
  top: "add_1:0"
    scale_param {
    "InputShapes": "52x52x128, 52x52x128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_68_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_68_bias:0"
    "RuntimeMemory": "338.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_73"
  type: "Convolution"
  bottom: "add_1:0"
  top: "leaky_re_lu_8:0"
    scale_param {
    "InputShapes": "52x52x128, 5x5x1x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_73_kernel:0"
    "ConstInputs": "leaky_re_lu_8_slope:0"
    "ConstInputs": "keras_quantization_wrapper_73_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "676"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_8"
  type: "Prelu"
  bottom: "leaky_re_lu_8:0"
  top: "leaky_re_lu_8:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_73_kernel:0"
    "ConstInputs": "leaky_re_lu_8_slope:0"
    "ConstInputs": "keras_quantization_wrapper_73_bias:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_72"
  type: "Convolution"
  bottom: "add_1:0"
  top: "leaky_re_lu_20:0"
    scale_param {
    "InputShapes": "52x52x128, 5x5x1x128, 128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_72_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_72_bias:0"
    "ConstInputs": "leaky_re_lu_20_slope:0"
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "674"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_20"
  type: "Prelu"
  bottom: "leaky_re_lu_20:0"
  top: "leaky_re_lu_20:0"
    scale_param {
    "InputShapes": "52x52x128, 1"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_72_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_72_bias:0"
    "ConstInputs": "leaky_re_lu_20_slope:0"
    "RuntimeMemory": "338.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_75"
  type: "Convolution"
  bottom: "leaky_re_lu_8:0"
  top: "leaky_re_lu_9:0"
    scale_param {
    "InputShapes": "26x26x128, 1x1x128x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_75_kernel:0"
    "ConstInputs": "leaky_re_lu_9_slope:0"
    "ConstInputs": "keras_quantization_wrapper_75_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "679"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_9"
  type: "Prelu"
  bottom: "leaky_re_lu_9:0"
  top: "leaky_re_lu_9:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_75_kernel:0"
    "ConstInputs": "leaky_re_lu_9_slope:0"
    "ConstInputs": "keras_quantization_wrapper_75_bias:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_74"
  type: "Convolution"
  bottom: "leaky_re_lu_20:0"
  top: "leaky_re_lu_21:0"
    scale_param {
    "InputShapes": "52x52x128, 1x1x128x128, 128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_74_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_74_bias:0"
    "ConstInputs": "leaky_re_lu_21_slope:0"
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "678"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_21"
  type: "Prelu"
  bottom: "leaky_re_lu_21:0"
  top: "leaky_re_lu_21:0"
    scale_param {
    "InputShapes": "52x52x128, 1"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_74_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_74_bias:0"
    "ConstInputs": "leaky_re_lu_21_slope:0"
    "RuntimeMemory": "338.00KB"
    
  }

}
layer {
  name: "Concat: n3_input"
  type: "Concat"
  bottom: "add:0"
  bottom: "leaky_re_lu_9:0"
  top: "n3_input:0"
    scale_param {
    "InputShapes": "26x26x128, 26x26x128"
    "OutputShape": "26x26x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "684"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_76"
  type: "Convolution"
  bottom: "leaky_re_lu_21:0"
  top: "leaky_re_lu_22:0"
    scale_param {
    "InputShapes": "52x52x128, 5x5x1x128, 128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_76_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_76_bias:0"
    "ConstInputs": "leaky_re_lu_22_slope:0"
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "680"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_22"
  type: "Prelu"
  bottom: "leaky_re_lu_22:0"
  top: "leaky_re_lu_22:0"
    scale_param {
    "InputShapes": "52x52x128, 1"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_76_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_76_bias:0"
    "ConstInputs": "leaky_re_lu_22_slope:0"
    "RuntimeMemory": "338.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_77"
  type: "Convolution"
  bottom: "n3_input:0"
  top: "keras_quantization_wrapper_77:0"
    scale_param {
    "InputShapes": "26x26x256, 5x5x1x256, 256"
    "OutputShape": "26x26x256"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_77_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_77_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "685"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 256
    }
  }
  blobs {
    shape {
      dim: 256
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_78"
  type: "Convolution"
  bottom: "n3_input:0"
  top: "leaky_re_lu_10:0"
    scale_param {
    "InputShapes": "26x26x256, 1x1x256x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_78_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_78_bias:0"
    "ConstInputs": "leaky_re_lu_10_slope:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "687"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_10"
  type: "Prelu"
  bottom: "leaky_re_lu_10:0"
  top: "leaky_re_lu_10:0"
    scale_param {
    "InputShapes": "26x26x64, 1"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_78_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_78_bias:0"
    "ConstInputs": "leaky_re_lu_10_slope:0"
    "RuntimeMemory": "42.25KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_79"
  type: "Convolution"
  bottom: "leaky_re_lu_22:0"
  top: "leaky_re_lu_23:0"
    scale_param {
    "InputShapes": "52x52x128, 1x1x128x128, 128"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_79_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_79_bias:0"
    "ConstInputs": "leaky_re_lu_23_slope:0"
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "682"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_23"
  type: "Prelu"
  bottom: "leaky_re_lu_23:0"
  top: "leaky_re_lu_23:0"
    scale_param {
    "InputShapes": "52x52x128, 1"
    "OutputShape": "52x52x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_79_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_79_bias:0"
    "ConstInputs": "leaky_re_lu_23_slope:0"
    "RuntimeMemory": "338.00KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_81"
  type: "Convolution"
  bottom: "leaky_re_lu_10:0"
  top: "leaky_re_lu_11:0"
    scale_param {
    "InputShapes": "26x26x64, 3x3x1x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_81_kernel:0"
    "ConstInputs": "leaky_re_lu_11_slope:0"
    "ConstInputs": "keras_quantization_wrapper_81_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "688"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_11"
  type: "Prelu"
  bottom: "leaky_re_lu_11:0"
  top: "leaky_re_lu_11:0"
    scale_param {
    "InputShapes": "26x26x64, 1"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_81_kernel:0"
    "ConstInputs": "leaky_re_lu_11_slope:0"
    "ConstInputs": "keras_quantization_wrapper_81_bias:0"
    "RuntimeMemory": "42.25KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_82"
  type: "Convolution"
  bottom: "leaky_re_lu_23:0"
  top: "keras_quantization_wrapper_82:0"
    scale_param {
    "InputShapes": "52x52x128, 1x1x128x35, 35"
    "OutputShape": "52x52x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_82_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_82_bias:0"
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "683"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 35
    }
  }
  blobs {
    shape {
      dim: 35
    }
  }
}
layer {
  name: "Concat: concatenate_4"
  type: "Concat"
  bottom: "leaky_re_lu_11:0"
  bottom: "leaky_re_lu_10:0"
  top: "concatenate_4:0"
    scale_param {
    "InputShapes": "26x26x64, 26x26x64"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "690"

  }

}
layer {
  name: "Transform: transform-140-keras_quantization_wrapper_82"
  type: "Transform"
  bottom: "keras_quantization_wrapper_82:0"
  top: "transform-140-keras_quantization_wrapper_82:0"
    scale_param {
    "InputShapes": "52x52x35"
    "OutputShape": "52x52x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "113.75KB"
    "ScheduleId": "766"

  }

}
layer {
  name: "Transform: transform-141-keras_quantization_wrapper_82"
  type: "Transform"
  bottom: "keras_quantization_wrapper_82:0"
  top: "transform-141-keras_quantization_wrapper_82:0"
    scale_param {
    "InputShapes": "52x52x35"
    "OutputShape": "52x52x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "113.75KB"
    "ScheduleId": "771"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_83"
  type: "Convolution"
  bottom: "concatenate_4:0"
  top: "keras_quantization_wrapper_83:0"
    scale_param {
    "InputShapes": "26x26x128, 1x1x128x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_83_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_83_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "691"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "StrideSlice: tf.splitsts0"
  type: "StrideSlice"
  bottom: "transform-140-keras_quantization_wrapper_82:0"
  top: "tf.splitsts0:0"
    scale_param {
    "InputShapes": "52x52x35"
    "OutputShape": "52x52x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "9.75KB"
    "ScheduleId": "767"

  }

}
layer {
  name: "StrideSlice: tf.splitsts1"
  type: "StrideSlice"
  bottom: "transform-141-keras_quantization_wrapper_82:0"
  top: "tf.splitsts1:0"
    scale_param {
    "InputShapes": "52x52x35"
    "OutputShape": "52x52x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "104.00KB"
    "ScheduleId": "772"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_84"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_83:0"
  top: "keras_quantization_wrapper_84:0"
    scale_param {
    "InputShapes": "26x26x64, 3x3x1x64, 64"
    "OutputShape": "26x26x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_84_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_84_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "692"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Transform: transform-120-tf.splitsts0"
  type: "Transform"
  bottom: "tf.splitsts0:0"
  top: "transform-120-tf.splitsts0:0"
    scale_param {
    "InputShapes": "52x52x3"
    "OutputShape": "52x52x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "768"

  }

}
layer {
  name: "Transform: transform-119-tf.splitsts1"
  type: "Transform"
  bottom: "tf.splitsts1:0"
  top: "transform-119-tf.splitsts1:0"
    scale_param {
    "InputShapes": "52x52x32"
    "OutputShape": "52x52x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "773"

  }

}
layer {
  name: "Concat: concatenate_5"
  type: "Concat"
  bottom: "keras_quantization_wrapper_83:0"
  bottom: "keras_quantization_wrapper_84:0"
  top: "concatenate_5:0"
    scale_param {
    "InputShapes": "26x26x64, 26x26x64"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "694"

  }

}
layer {
  name: "Reshape: tf.reshape_34"
  type: "Reshape"
  bottom: "transform-120-tf.splitsts0:0"
  top: "tf.reshape_34:0"
    scale_param {
    "InputShapes": "52x52x3"
    "OutputShape": "2704x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "85.00KB"
    "ScheduleId": "769"

  }

}
layer {
  name: "Reshape: tf.reshape_33"
  type: "Reshape"
  bottom: "transform-119-tf.splitsts1:0"
  top: "tf.reshape_33:0"
    scale_param {
    "InputShapes": "52x52x32"
    "OutputShape": "2704x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "338.00KB"
    "ScheduleId": "774"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_80"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_77:0"
  bottom: "concatenate_5:0"
  top: "add_2:0"
    scale_param {
    "InputShapes": "26x26x256, 1x1x256x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_80_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_80_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "695"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Add: add_2"
  type: "Add"
  bottom: "add_2:0"
  top: "add_2:0"
    scale_param {
    "InputShapes": "26x26x128, 26x26x128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_80_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_80_bias:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Transform: transform-49-tf.reshape_34"
  type: "Transform"
  bottom: "tf.reshape_34:0"
  top: "transform-49-tf.reshape_34:0"
    scale_param {
    "InputShapes": "2704x3"
    "OutputShape": "2704x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "7.97KB"
    "ScheduleId": "776"

  }

}
layer {
  name: "Transform: transform-68-tf.reshape_33"
  type: "Transform"
  bottom: "tf.reshape_33:0"
  top: "transform-68-tf.reshape_33:0"
    scale_param {
    "InputShapes": "2704x4x8"
    "OutputShape": "2704x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "85.00KB"
    "ScheduleId": "782"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_87"
  type: "Convolution"
  bottom: "add_2:0"
  top: "leaky_re_lu_12:0"
    scale_param {
    "InputShapes": "26x26x128, 5x5x1x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_87_kernel:0"
    "ConstInputs": "leaky_re_lu_12_slope:0"
    "ConstInputs": "keras_quantization_wrapper_87_bias:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "698"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_12"
  type: "Prelu"
  bottom: "leaky_re_lu_12:0"
  top: "leaky_re_lu_12:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_87_kernel:0"
    "ConstInputs": "leaky_re_lu_12_slope:0"
    "ConstInputs": "keras_quantization_wrapper_87_bias:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_86"
  type: "Convolution"
  bottom: "add_2:0"
  top: "leaky_re_lu_24:0"
    scale_param {
    "InputShapes": "26x26x128, 5x5x1x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_86_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_86_bias:0"
    "ConstInputs": "leaky_re_lu_24_slope:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "696"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_24"
  type: "Prelu"
  bottom: "leaky_re_lu_24:0"
  top: "leaky_re_lu_24:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_86_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_86_bias:0"
    "ConstInputs": "leaky_re_lu_24_slope:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Softmax: softmax"
  type: "Softmax"
  bottom: "transform-68-tf.reshape_33:0"
  top: "softmax:0"
    scale_param {
    "InputShapes": "2704x4x8"
    "OutputShape": "2704x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "85.00KB"
    "ScheduleId": "783"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_89"
  type: "Convolution"
  bottom: "leaky_re_lu_12:0"
  top: "leaky_re_lu_13:0"
    scale_param {
    "InputShapes": "13x13x128, 1x1x128x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_89_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_89_bias:0"
    "ConstInputs": "leaky_re_lu_13_slope:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "701"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_13"
  type: "Prelu"
  bottom: "leaky_re_lu_13:0"
  top: "leaky_re_lu_13:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_89_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_89_bias:0"
    "ConstInputs": "leaky_re_lu_13_slope:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_88"
  type: "Convolution"
  bottom: "leaky_re_lu_24:0"
  top: "leaky_re_lu_25:0"
    scale_param {
    "InputShapes": "26x26x128, 1x1x128x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_88_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_88_bias:0"
    "ConstInputs": "leaky_re_lu_25_slope:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "700"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_25"
  type: "Prelu"
  bottom: "leaky_re_lu_25:0"
  top: "leaky_re_lu_25:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_88_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_88_bias:0"
    "ConstInputs": "leaky_re_lu_25_slope:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Transform: transform-57-softmax"
  type: "Transform"
  bottom: "softmax:0"
  top: "transform-57-softmax:0"
    scale_param {
    "InputShapes": "2704x4x8"
    "OutputShape": "2704x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "676.00KB"
    "ScheduleId": "784"

  }

}
layer {
  name: "Concat: n4_input"
  type: "Concat"
  bottom: "leaky_re_lu_3:0"
  bottom: "leaky_re_lu_13:0"
  top: "n4_input:0"
    scale_param {
    "InputShapes": "13x13x128, 13x13x128"
    "OutputShape": "13x13x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "706"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_90"
  type: "Convolution"
  bottom: "leaky_re_lu_25:0"
  top: "leaky_re_lu_26:0"
    scale_param {
    "InputShapes": "26x26x128, 5x5x1x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_90_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_90_bias:0"
    "ConstInputs": "leaky_re_lu_26_slope:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "702"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_26"
  type: "Prelu"
  bottom: "leaky_re_lu_26:0"
  top: "leaky_re_lu_26:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_90_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_90_bias:0"
    "ConstInputs": "leaky_re_lu_26_slope:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_85"
  type: "Convolution"
  bottom: "transform-57-softmax:0"
  top: "keras_quantization_wrapper_85:0"
    scale_param {
    "InputShapes": "2704x4x8, 1x1x8x1, 1"
    "OutputShape": "2704x4x1"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_85_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_85_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "785"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 8
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_91"
  type: "Convolution"
  bottom: "n4_input:0"
  top: "keras_quantization_wrapper_91:0"
    scale_param {
    "InputShapes": "13x13x256, 5x5x1x256, 256"
    "OutputShape": "13x13x256"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_91_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_91_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "707"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 256
    }
  }
  blobs {
    shape {
      dim: 256
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_92"
  type: "Convolution"
  bottom: "n4_input:0"
  top: "leaky_re_lu_14:0"
    scale_param {
    "InputShapes": "13x13x256, 1x1x256x64, 64"
    "OutputShape": "13x13x64"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_92_kernel:0"
    "ConstInputs": "leaky_re_lu_14_slope:0"
    "ConstInputs": "keras_quantization_wrapper_92_bias:0"
    "RuntimeMemory": "10.56KB"
    "ScheduleId": "709"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_14"
  type: "Prelu"
  bottom: "leaky_re_lu_14:0"
  top: "leaky_re_lu_14:0"
    scale_param {
    "InputShapes": "13x13x64, 1"
    "OutputShape": "13x13x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_92_kernel:0"
    "ConstInputs": "leaky_re_lu_14_slope:0"
    "ConstInputs": "keras_quantization_wrapper_92_bias:0"
    "RuntimeMemory": "10.56KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_93"
  type: "Convolution"
  bottom: "leaky_re_lu_26:0"
  top: "leaky_re_lu_27:0"
    scale_param {
    "InputShapes": "26x26x128, 1x1x128x128, 128"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_93_kernel:0"
    "ConstInputs": "leaky_re_lu_27_slope:0"
    "ConstInputs": "keras_quantization_wrapper_93_bias:0"
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "704"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_27"
  type: "Prelu"
  bottom: "leaky_re_lu_27:0"
  top: "leaky_re_lu_27:0"
    scale_param {
    "InputShapes": "26x26x128, 1"
    "OutputShape": "26x26x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_93_kernel:0"
    "ConstInputs": "leaky_re_lu_27_slope:0"
    "ConstInputs": "keras_quantization_wrapper_93_bias:0"
    "RuntimeMemory": "84.50KB"
    
  }

}
layer {
  name: "Squeeze: tf.compat.v1.squeeze"
  type: "Squeeze"
  bottom: "keras_quantization_wrapper_85:0"
  top: "tf.compat.v1.squeeze:0"
    scale_param {
    "InputShapes": "2704x4x1"
    "OutputShape": "2704x4"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "84.50KB"
    "ScheduleId": "787"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_95"
  type: "Convolution"
  bottom: "leaky_re_lu_14:0"
  top: "leaky_re_lu_15:0"
    scale_param {
    "InputShapes": "13x13x64, 3x3x1x64, 64"
    "OutputShape": "13x13x64"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_95_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_95_bias:0"
    "ConstInputs": "leaky_re_lu_15_slope:0"
    "RuntimeMemory": "10.56KB"
    "ScheduleId": "710"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_15"
  type: "Prelu"
  bottom: "leaky_re_lu_15:0"
  top: "leaky_re_lu_15:0"
    scale_param {
    "InputShapes": "13x13x64, 1"
    "OutputShape": "13x13x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_95_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_95_bias:0"
    "ConstInputs": "leaky_re_lu_15_slope:0"
    "RuntimeMemory": "10.56KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_96"
  type: "Convolution"
  bottom: "leaky_re_lu_27:0"
  top: "keras_quantization_wrapper_96:0"
    scale_param {
    "InputShapes": "26x26x128, 1x1x128x35, 35"
    "OutputShape": "26x26x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_96_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_96_bias:0"
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "705"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 35
    }
  }
  blobs {
    shape {
      dim: 35
    }
  }
}
layer {
  name: "Transform: transform-90-tf.compat.v1.squeeze"
  type: "Transform"
  bottom: "tf.compat.v1.squeeze:0"
  top: "transform-90-tf.compat.v1.squeeze:0"
    scale_param {
    "InputShapes": "2704x4"
    "OutputShape": "2704x4"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "10.63KB"
    "ScheduleId": "807"

  }

}
layer {
  name: "Concat: concatenate_6"
  type: "Concat"
  bottom: "leaky_re_lu_15:0"
  bottom: "leaky_re_lu_14:0"
  top: "concatenate_6:0"
    scale_param {
    "InputShapes": "13x13x64, 13x13x64"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "712"

  }

}
layer {
  name: "Transform: transform-134-keras_quantization_wrapper_96"
  type: "Transform"
  bottom: "keras_quantization_wrapper_96:0"
  top: "transform-134-keras_quantization_wrapper_96:0"
    scale_param {
    "InputShapes": "26x26x35"
    "OutputShape": "26x26x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "28.44KB"
    "ScheduleId": "736"

  }

}
layer {
  name: "Transform: transform-135-keras_quantization_wrapper_96"
  type: "Transform"
  bottom: "keras_quantization_wrapper_96:0"
  top: "transform-135-keras_quantization_wrapper_96:0"
    scale_param {
    "InputShapes": "26x26x35"
    "OutputShape": "26x26x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "28.44KB"
    "ScheduleId": "741"

  }

}
layer {
  name: "Mul: tf.math.multiply"
  type: "Mul"
  bottom: "transform-90-tf.compat.v1.squeeze:0"
  top: "tf.math.multiply:0"
    scale_param {
    "InputShapes": "2704x4, 2704x1"
    "OutputShape": "2704x4"
    "Quantize[mn,mx]": [0, 0.25]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.multiply_y:0"
    "RuntimeMemory": "10.63KB"
    "ScheduleId": "808"

  }
  blobs {
    shape {
      dim: 2704
      dim: 1
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_97"
  type: "Convolution"
  bottom: "concatenate_6:0"
  top: "keras_quantization_wrapper_97:0"
    scale_param {
    "InputShapes": "13x13x128, 1x1x128x64, 64"
    "OutputShape": "13x13x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_97_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_97_bias:0"
    "RuntimeMemory": "10.56KB"
    "ScheduleId": "713"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "StrideSlice: tf.split_1sts0"
  type: "StrideSlice"
  bottom: "transform-134-keras_quantization_wrapper_96:0"
  top: "tf.split_1sts0:0"
    scale_param {
    "InputShapes": "26x26x35"
    "OutputShape": "26x26x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.44KB"
    "ScheduleId": "737"

  }

}
layer {
  name: "StrideSlice: tf.split_1sts1"
  type: "StrideSlice"
  bottom: "transform-135-keras_quantization_wrapper_96:0"
  top: "tf.split_1sts1:0"
    scale_param {
    "InputShapes": "26x26x35"
    "OutputShape": "26x26x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "26.00KB"
    "ScheduleId": "742"

  }

}
layer {
  name: "Unstack: tf.unstack"
  type: "Unstack"
  bottom: "tf.math.multiply:0"
  top: "tf.unstack:3:3"
  top: "tf.unstack:2:2"
  top: "tf.unstack:1:1"
  top: "tf.unstack:0"
    scale_param {
    "InputShapes": "2704x4"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 0.25]
    "QuantizeBits": 8
    
    "RuntimeMemory": "10.63KB"
    "ScheduleId": "809"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_98"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_97:0"
  top: "keras_quantization_wrapper_98:0"
    scale_param {
    "InputShapes": "13x13x64, 3x3x1x64, 64"
    "OutputShape": "13x13x64"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_98_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_98_bias:0"
    "RuntimeMemory": "10.56KB"
    "ScheduleId": "714"

  }
  blobs {
    shape {
      dim: 3
      dim: 3
      dim: 1
      dim: 64
    }
  }
  blobs {
    shape {
      dim: 64
    }
  }
}
layer {
  name: "Transform: transform-122-tf.split_1sts0"
  type: "Transform"
  bottom: "tf.split_1sts0:0"
  top: "transform-122-tf.split_1sts0:0"
    scale_param {
    "InputShapes": "26x26x3"
    "OutputShape": "26x26x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "738"

  }

}
layer {
  name: "Transform: transform-121-tf.split_1sts1"
  type: "Transform"
  bottom: "tf.split_1sts1:0"
  top: "transform-121-tf.split_1sts1:0"
    scale_param {
    "InputShapes": "26x26x32"
    "OutputShape": "26x26x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "743"

  }

}
layer {
  name: "Sub: tf.math.subtract"
  type: "Sub"
  bottom: "tf.unstack:0"
  top: "re_lu:0"
    scale_param {
    "InputShapes": "2704, 2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_x:0"
    "RuntimeMemory": "2.66KB"
    "ScheduleId": "812"

  }
  blobs {
    shape {
      dim: 2704
    }
  }
}
layer {
  name: "Relu: re_lu"
  type: "Relu"
  bottom: "re_lu:0"
  top: "re_lu:0"
    scale_param {
    "InputShapes": "2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_x:0"
    "RuntimeMemory": "2.66KB"
    
  }

}
layer {
  name: "Sub: tf.math.subtract_1"
  type: "Sub"
  bottom: "tf.unstack:1:1"
  top: "re_lu_1:0"
    scale_param {
    "InputShapes": "2704, 2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_1_x:0"
    "RuntimeMemory": "2.66KB"
    "ScheduleId": "813"

  }
  blobs {
    shape {
      dim: 2704
    }
  }
}
layer {
  name: "Relu: re_lu_1"
  type: "Relu"
  bottom: "re_lu_1:0"
  top: "re_lu_1:0"
    scale_param {
    "InputShapes": "2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_1_x:0"
    "RuntimeMemory": "2.66KB"
    
  }

}
layer {
  name: "Add: tf.math.add"
  type: "Add"
  bottom: "tf.unstack:2:2"
  top: "re_lu_2:0"
    scale_param {
    "InputShapes": "2704, 2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_x:0"
    "RuntimeMemory": "2.66KB"
    "ScheduleId": "810"

  }
  blobs {
    shape {
      dim: 2704
    }
  }
}
layer {
  name: "Relu: re_lu_2"
  type: "Relu"
  bottom: "re_lu_2:0"
  top: "re_lu_2:0"
    scale_param {
    "InputShapes": "2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_x:0"
    "RuntimeMemory": "2.66KB"
    
  }

}
layer {
  name: "Add: tf.math.add_1"
  type: "Add"
  bottom: "tf.unstack:3:3"
  top: "re_lu_3:0"
    scale_param {
    "InputShapes": "2704, 2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_1_x:0"
    "RuntimeMemory": "2.66KB"
    "ScheduleId": "811"

  }
  blobs {
    shape {
      dim: 2704
    }
  }
}
layer {
  name: "Relu: re_lu_3"
  type: "Relu"
  bottom: "re_lu_3:0"
  top: "re_lu_3:0"
    scale_param {
    "InputShapes": "2704"
    "OutputShape": "2704"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_1_x:0"
    "RuntimeMemory": "2.66KB"
    
  }

}
layer {
  name: "Concat: concatenate_7"
  type: "Concat"
  bottom: "keras_quantization_wrapper_98:0"
  bottom: "keras_quantization_wrapper_97:0"
  top: "concatenate_7:0"
    scale_param {
    "InputShapes": "13x13x64, 13x13x64"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "716"

  }

}
layer {
  name: "Reshape: tf.reshape_37"
  type: "Reshape"
  bottom: "transform-122-tf.split_1sts0:0"
  top: "tf.reshape_37:0"
    scale_param {
    "InputShapes": "26x26x3"
    "OutputShape": "676x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "22.00KB"
    "ScheduleId": "739"

  }

}
layer {
  name: "Reshape: tf.reshape_36"
  type: "Reshape"
  bottom: "transform-121-tf.split_1sts1:0"
  top: "tf.reshape_36:0"
    scale_param {
    "InputShapes": "26x26x32"
    "OutputShape": "676x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "85.00KB"
    "ScheduleId": "744"

  }

}
layer {
  name: "Stack: tf.stack"
  type: "Stack"
  bottom: "re_lu:0"
  bottom: "re_lu_3:0"
  bottom: "re_lu_1:0"
  bottom: "re_lu_2:0"
  top: "tf.stack:0"
    scale_param {
    "InputShapes": "2704, 2704, 2704, 2704"
    "OutputShape": "2704x4"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "10.63KB"
    "ScheduleId": "814"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_94"
  type: "Convolution"
  bottom: "keras_quantization_wrapper_91:0"
  bottom: "concatenate_7:0"
  top: "add_3:0"
    scale_param {
    "InputShapes": "13x13x256, 1x1x256x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_94_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_94_bias:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "717"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 256
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Add: add_3"
  type: "Add"
  bottom: "add_3:0"
  top: "add_3:0"
    scale_param {
    "InputShapes": "13x13x128, 13x13x128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_94_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_94_bias:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Transform: transform-50-tf.reshape_37"
  type: "Transform"
  bottom: "tf.reshape_37:0"
  top: "transform-50-tf.reshape_37:0"
    scale_param {
    "InputShapes": "676x3"
    "OutputShape": "676x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.06KB"
    "ScheduleId": "777"

  }

}
layer {
  name: "Transform: transform-69-tf.reshape_36"
  type: "Transform"
  bottom: "tf.reshape_36:0"
  top: "transform-69-tf.reshape_36:0"
    scale_param {
    "InputShapes": "676x4x8"
    "OutputShape": "676x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "22.00KB"
    "ScheduleId": "788"

  }

}
layer {
  name: "Reshape: tf.expand_dims_reshaped"
  type: "Reshape"
  bottom: "tf.stack:0"
  top: "tf.expand_dims_reshaped:0"
    scale_param {
    "InputShapes": "2704x4"
    "OutputShape": "2704x1x4"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "10.63KB"
    "ScheduleId": "815"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_101"
  type: "Convolution"
  bottom: "add_3:0"
  top: "leaky_re_lu_16:0"
    scale_param {
    "InputShapes": "13x13x128, 5x5x1x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_101_kernel:0"
    "ConstInputs": "leaky_re_lu_16_slope:0"
    "ConstInputs": "keras_quantization_wrapper_101_bias:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "720"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_16"
  type: "Prelu"
  bottom: "leaky_re_lu_16:0"
  top: "leaky_re_lu_16:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_101_kernel:0"
    "ConstInputs": "leaky_re_lu_16_slope:0"
    "ConstInputs": "keras_quantization_wrapper_101_bias:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_100"
  type: "Convolution"
  bottom: "add_3:0"
  top: "leaky_re_lu_28:0"
    scale_param {
    "InputShapes": "13x13x128, 5x5x1x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_100_kernel:0"
    "ConstInputs": "leaky_re_lu_28_slope:0"
    "ConstInputs": "keras_quantization_wrapper_100_bias:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "718"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_28"
  type: "Prelu"
  bottom: "leaky_re_lu_28:0"
  top: "leaky_re_lu_28:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_100_kernel:0"
    "ConstInputs": "leaky_re_lu_28_slope:0"
    "ConstInputs": "keras_quantization_wrapper_100_bias:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Softmax: softmax_1"
  type: "Softmax"
  bottom: "transform-69-tf.reshape_36:0"
  top: "softmax_1:0"
    scale_param {
    "InputShapes": "676x4x8"
    "OutputShape": "676x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "22.00KB"
    "ScheduleId": "789"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_103"
  type: "Convolution"
  bottom: "leaky_re_lu_16:0"
  top: "leaky_re_lu_17:0"
    scale_param {
    "InputShapes": "7x7x128, 1x1x128x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_103_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_103_bias:0"
    "ConstInputs": "leaky_re_lu_17_slope:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "723"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_17"
  type: "Prelu"
  bottom: "leaky_re_lu_17:0"
  top: "leaky_re_lu_17:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_103_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_103_bias:0"
    "ConstInputs": "leaky_re_lu_17_slope:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_102"
  type: "Convolution"
  bottom: "leaky_re_lu_28:0"
  top: "leaky_re_lu_29:0"
    scale_param {
    "InputShapes": "13x13x128, 1x1x128x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_102_kernel:0"
    "ConstInputs": "leaky_re_lu_29_slope:0"
    "ConstInputs": "keras_quantization_wrapper_102_bias:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "722"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_29"
  type: "Prelu"
  bottom: "leaky_re_lu_29:0"
  top: "leaky_re_lu_29:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_102_kernel:0"
    "ConstInputs": "leaky_re_lu_29_slope:0"
    "ConstInputs": "keras_quantization_wrapper_102_bias:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Transform: transform-58-softmax_1"
  type: "Transform"
  bottom: "softmax_1:0"
  top: "transform-58-softmax_1:0"
    scale_param {
    "InputShapes": "676x4x8"
    "OutputShape": "676x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "169.00KB"
    "ScheduleId": "790"

  }

}
layer {
  name: "Add: add_4"
  type: "Add"
  bottom: "leaky_re_lu_19:0"
  bottom: "leaky_re_lu_17:0"
  top: "add_4:0"
    scale_param {
    "InputShapes": "7x7x128, 7x7x128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "724"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_104"
  type: "Convolution"
  bottom: "leaky_re_lu_29:0"
  top: "leaky_re_lu_30:0"
    scale_param {
    "InputShapes": "13x13x128, 5x5x1x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_104_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_104_bias:0"
    "ConstInputs": "leaky_re_lu_30_slope:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "725"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_30"
  type: "Prelu"
  bottom: "leaky_re_lu_30:0"
  top: "leaky_re_lu_30:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_104_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_104_bias:0"
    "ConstInputs": "leaky_re_lu_30_slope:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_99"
  type: "Convolution"
  bottom: "transform-58-softmax_1:0"
  top: "keras_quantization_wrapper_99:0"
    scale_param {
    "InputShapes": "676x4x8, 1x1x8x1, 1"
    "OutputShape": "676x4x1"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_99_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_99_bias:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "791"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 8
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_105"
  type: "Convolution"
  bottom: "add_4:0"
  top: "leaky_re_lu_32:0"
    scale_param {
    "InputShapes": "7x7x128, 5x5x1x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_105_kernel:0"
    "ConstInputs": "leaky_re_lu_32_slope:0"
    "ConstInputs": "keras_quantization_wrapper_105_bias:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "727"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_32"
  type: "Prelu"
  bottom: "leaky_re_lu_32:0"
  top: "leaky_re_lu_32:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_105_kernel:0"
    "ConstInputs": "leaky_re_lu_32_slope:0"
    "ConstInputs": "keras_quantization_wrapper_105_bias:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_106"
  type: "Convolution"
  bottom: "leaky_re_lu_30:0"
  top: "leaky_re_lu_31:0"
    scale_param {
    "InputShapes": "13x13x128, 1x1x128x128, 128"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-256.0, 256.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_106_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_106_bias:0"
    "ConstInputs": "leaky_re_lu_31_slope:0"
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "729"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_31"
  type: "Prelu"
  bottom: "leaky_re_lu_31:0"
  top: "leaky_re_lu_31:0"
    scale_param {
    "InputShapes": "13x13x128, 1"
    "OutputShape": "13x13x128"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_106_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_106_bias:0"
    "ConstInputs": "leaky_re_lu_31_slope:0"
    "RuntimeMemory": "21.13KB"
    
  }

}
layer {
  name: "Squeeze: tf.compat.v1.squeeze_1"
  type: "Squeeze"
  bottom: "keras_quantization_wrapper_99:0"
  top: "tf.compat.v1.squeeze_1:0"
    scale_param {
    "InputShapes": "676x4x1"
    "OutputShape": "676x4"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "21.13KB"
    "ScheduleId": "793"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_107"
  type: "Convolution"
  bottom: "leaky_re_lu_32:0"
  top: "leaky_re_lu_33:0"
    scale_param {
    "InputShapes": "7x7x128, 1x1x128x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_107_kernel:0"
    "ConstInputs": "leaky_re_lu_33_slope:0"
    "ConstInputs": "keras_quantization_wrapper_107_bias:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "730"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_33"
  type: "Prelu"
  bottom: "leaky_re_lu_33:0"
  top: "leaky_re_lu_33:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_107_kernel:0"
    "ConstInputs": "leaky_re_lu_33_slope:0"
    "ConstInputs": "keras_quantization_wrapper_107_bias:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_108"
  type: "Convolution"
  bottom: "leaky_re_lu_31:0"
  top: "keras_quantization_wrapper_108:0"
    scale_param {
    "InputShapes": "13x13x128, 1x1x128x35, 35"
    "OutputShape": "13x13x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_108_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_108_bias:0"
    "RuntimeMemory": "10.56KB"
    "ScheduleId": "731"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 35
    }
  }
  blobs {
    shape {
      dim: 35
    }
  }
}
layer {
  name: "Transform: transform-91-tf.compat.v1.squeeze_1"
  type: "Transform"
  bottom: "tf.compat.v1.squeeze_1:0"
  top: "transform-91-tf.compat.v1.squeeze_1:0"
    scale_param {
    "InputShapes": "676x4"
    "OutputShape": "676x4"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.75KB"
    "ScheduleId": "817"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_109"
  type: "Convolution"
  bottom: "leaky_re_lu_33:0"
  top: "leaky_re_lu_34:0"
    scale_param {
    "InputShapes": "7x7x128, 5x5x1x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_109_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_109_bias:0"
    "ConstInputs": "leaky_re_lu_34_slope:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "732"

  }
  blobs {
    shape {
      dim: 5
      dim: 5
      dim: 1
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_34"
  type: "Prelu"
  bottom: "leaky_re_lu_34:0"
  top: "leaky_re_lu_34:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_109_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_109_bias:0"
    "ConstInputs": "leaky_re_lu_34_slope:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "Transform: transform-136-keras_quantization_wrapper_108"
  type: "Transform"
  bottom: "keras_quantization_wrapper_108:0"
  top: "transform-136-keras_quantization_wrapper_108:0"
    scale_param {
    "InputShapes": "13x13x35"
    "OutputShape": "13x13x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "14.22KB"
    "ScheduleId": "746"

  }

}
layer {
  name: "Transform: transform-137-keras_quantization_wrapper_108"
  type: "Transform"
  bottom: "keras_quantization_wrapper_108:0"
  top: "transform-137-keras_quantization_wrapper_108:0"
    scale_param {
    "InputShapes": "13x13x35"
    "OutputShape": "13x13x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "14.22KB"
    "ScheduleId": "751"

  }

}
layer {
  name: "Mul: tf.math.multiply_1"
  type: "Mul"
  bottom: "transform-91-tf.compat.v1.squeeze_1:0"
  top: "tf.math.multiply_1:0"
    scale_param {
    "InputShapes": "676x4, 676x1"
    "OutputShape": "676x4"
    "Quantize[mn,mx]": [0, 0.5]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.multiply_1_y:0"
    "RuntimeMemory": "2.75KB"
    "ScheduleId": "818"

  }
  blobs {
    shape {
      dim: 676
      dim: 1
    }
  }
}
layer {
  name: "Conv2D: keras_quantization_wrapper_110"
  type: "Convolution"
  bottom: "leaky_re_lu_34:0"
  top: "leaky_re_lu_35:0"
    scale_param {
    "InputShapes": "7x7x128, 1x1x128x128, 128"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-128.0, 128.0]
    "QuantizeBits": 16
    "ConstInputs": "keras_quantization_wrapper_110_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_110_bias:0"
    "ConstInputs": "leaky_re_lu_35_slope:0"
    "RuntimeMemory": "6.13KB"
    "ScheduleId": "734"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 128
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Prelu: leaky_re_lu_35"
  type: "Prelu"
  bottom: "leaky_re_lu_35:0"
  top: "leaky_re_lu_35:0"
    scale_param {
    "InputShapes": "7x7x128, 1"
    "OutputShape": "7x7x128"
    "Quantize[mn,mx]": [-8.0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_110_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_110_bias:0"
    "ConstInputs": "leaky_re_lu_35_slope:0"
    "RuntimeMemory": "6.13KB"
    
  }

}
layer {
  name: "StrideSlice: tf.split_2sts0"
  type: "StrideSlice"
  bottom: "transform-136-keras_quantization_wrapper_108:0"
  top: "tf.split_2sts0:0"
    scale_param {
    "InputShapes": "13x13x35"
    "OutputShape": "13x13x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "1.22KB"
    "ScheduleId": "747"

  }

}
layer {
  name: "StrideSlice: tf.split_2sts1"
  type: "StrideSlice"
  bottom: "transform-137-keras_quantization_wrapper_108:0"
  top: "tf.split_2sts1:0"
    scale_param {
    "InputShapes": "13x13x35"
    "OutputShape": "13x13x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "13.00KB"
    "ScheduleId": "752"

  }

}
layer {
  name: "Unstack: tf.unstack_1"
  type: "Unstack"
  bottom: "tf.math.multiply_1:0"
  top: "tf.unstack_1:3:3"
  top: "tf.unstack_1:2:2"
  top: "tf.unstack_1:1:1"
  top: "tf.unstack_1:0"
    scale_param {
    "InputShapes": "676x4"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 0.5]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.75KB"
    "ScheduleId": "819"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_112"
  type: "Convolution"
  bottom: "leaky_re_lu_35:0"
  top: "keras_quantization_wrapper_112:0"
    scale_param {
    "InputShapes": "7x7x128, 1x1x128x35, 35"
    "OutputShape": "7x7x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_112_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_112_bias:0"
    "RuntimeMemory": "3.06KB"
    "ScheduleId": "735"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 128
      dim: 35
    }
  }
  blobs {
    shape {
      dim: 35
    }
  }
}
layer {
  name: "Transform: transform-125-tf.split_2sts0"
  type: "Transform"
  bottom: "tf.split_2sts0:0"
  top: "transform-125-tf.split_2sts0:0"
    scale_param {
    "InputShapes": "13x13x3"
    "OutputShape": "13x13x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "5.28KB"
    "ScheduleId": "748"

  }

}
layer {
  name: "Transform: transform-123-tf.split_2sts1"
  type: "Transform"
  bottom: "tf.split_2sts1:0"
  top: "transform-123-tf.split_2sts1:0"
    scale_param {
    "InputShapes": "13x13x32"
    "OutputShape": "13x13x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "5.28KB"
    "ScheduleId": "753"

  }

}
layer {
  name: "Sub: tf.math.subtract_2"
  type: "Sub"
  bottom: "tf.unstack_1:0"
  top: "re_lu_4:0"
    scale_param {
    "InputShapes": "676, 676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_2_x:0"
    "RuntimeMemory": "704B"
    "ScheduleId": "822"

  }
  blobs {
    shape {
      dim: 676
    }
  }
}
layer {
  name: "Relu: re_lu_4"
  type: "Relu"
  bottom: "re_lu_4:0"
  top: "re_lu_4:0"
    scale_param {
    "InputShapes": "676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_2_x:0"
    "RuntimeMemory": "704B"
    
  }

}
layer {
  name: "Sub: tf.math.subtract_3"
  type: "Sub"
  bottom: "tf.unstack_1:1:1"
  top: "re_lu_5:0"
    scale_param {
    "InputShapes": "676, 676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_3_x:0"
    "RuntimeMemory": "704B"
    "ScheduleId": "823"

  }
  blobs {
    shape {
      dim: 676
    }
  }
}
layer {
  name: "Relu: re_lu_5"
  type: "Relu"
  bottom: "re_lu_5:0"
  top: "re_lu_5:0"
    scale_param {
    "InputShapes": "676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_3_x:0"
    "RuntimeMemory": "704B"
    
  }

}
layer {
  name: "Add: tf.math.add_2"
  type: "Add"
  bottom: "tf.unstack_1:2:2"
  top: "re_lu_6:0"
    scale_param {
    "InputShapes": "676, 676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_2_x:0"
    "RuntimeMemory": "704B"
    "ScheduleId": "820"

  }
  blobs {
    shape {
      dim: 676
    }
  }
}
layer {
  name: "Relu: re_lu_6"
  type: "Relu"
  bottom: "re_lu_6:0"
  top: "re_lu_6:0"
    scale_param {
    "InputShapes": "676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_2_x:0"
    "RuntimeMemory": "704B"
    
  }

}
layer {
  name: "Add: tf.math.add_3"
  type: "Add"
  bottom: "tf.unstack_1:3:3"
  top: "re_lu_7:0"
    scale_param {
    "InputShapes": "676, 676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_3_x:0"
    "RuntimeMemory": "704B"
    "ScheduleId": "821"

  }
  blobs {
    shape {
      dim: 676
    }
  }
}
layer {
  name: "Relu: re_lu_7"
  type: "Relu"
  bottom: "re_lu_7:0"
  top: "re_lu_7:0"
    scale_param {
    "InputShapes": "676"
    "OutputShape": "676"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_3_x:0"
    "RuntimeMemory": "704B"
    
  }

}
layer {
  name: "Transform: transform-138-keras_quantization_wrapper_112"
  type: "Transform"
  bottom: "keras_quantization_wrapper_112:0"
  top: "transform-138-keras_quantization_wrapper_112:0"
    scale_param {
    "InputShapes": "7x7x35"
    "OutputShape": "7x7x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "7.66KB"
    "ScheduleId": "756"

  }

}
layer {
  name: "Transform: transform-139-keras_quantization_wrapper_112"
  type: "Transform"
  bottom: "keras_quantization_wrapper_112:0"
  top: "transform-139-keras_quantization_wrapper_112:0"
    scale_param {
    "InputShapes": "7x7x35"
    "OutputShape": "7x7x35"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "7.66KB"
    "ScheduleId": "761"

  }

}
layer {
  name: "Reshape: tf.reshape_40"
  type: "Reshape"
  bottom: "transform-125-tf.split_2sts0:0"
  top: "tf.reshape_40:0"
    scale_param {
    "InputShapes": "13x13x3"
    "OutputShape": "169x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "6.00KB"
    "ScheduleId": "749"

  }

}
layer {
  name: "Reshape: tf.reshape_39"
  type: "Reshape"
  bottom: "transform-123-tf.split_2sts1:0"
  top: "tf.reshape_39:0"
    scale_param {
    "InputShapes": "13x13x32"
    "OutputShape": "169x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "22.00KB"
    "ScheduleId": "754"

  }

}
layer {
  name: "Stack: tf.stack_1"
  type: "Stack"
  bottom: "re_lu_7:0"
  bottom: "re_lu_6:0"
  bottom: "re_lu_4:0"
  bottom: "re_lu_5:0"
  top: "tf.stack_1:0"
    scale_param {
    "InputShapes": "676, 676, 676, 676"
    "OutputShape": "676x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.75KB"
    "ScheduleId": "824"

  }

}
layer {
  name: "StrideSlice: tf.split_3sts0"
  type: "StrideSlice"
  bottom: "transform-138-keras_quantization_wrapper_112:0"
  top: "tf.split_3sts0:0"
    scale_param {
    "InputShapes": "7x7x35"
    "OutputShape": "7x7x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "672B"
    "ScheduleId": "757"

  }

}
layer {
  name: "StrideSlice: tf.split_3sts1"
  type: "StrideSlice"
  bottom: "transform-139-keras_quantization_wrapper_112:0"
  top: "tf.split_3sts1:0"
    scale_param {
    "InputShapes": "7x7x35"
    "OutputShape": "7x7x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "7.00KB"
    "ScheduleId": "762"

  }

}
layer {
  name: "Transform: transform-51-tf.reshape_40"
  type: "Transform"
  bottom: "tf.reshape_40:0"
  top: "transform-51-tf.reshape_40:0"
    scale_param {
    "InputShapes": "169x3"
    "OutputShape": "169x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "576B"
    "ScheduleId": "778"

  }

}
layer {
  name: "Transform: transform-70-tf.reshape_39"
  type: "Transform"
  bottom: "tf.reshape_39:0"
  top: "transform-70-tf.reshape_39:0"
    scale_param {
    "InputShapes": "169x4x8"
    "OutputShape": "169x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "6.00KB"
    "ScheduleId": "794"

  }

}
layer {
  name: "Reshape: tf.expand_dims_1_reshaped"
  type: "Reshape"
  bottom: "tf.stack_1:0"
  top: "tf.expand_dims_1_reshaped:0"
    scale_param {
    "InputShapes": "676x4"
    "OutputShape": "676x1x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "2.75KB"
    "ScheduleId": "825"

  }

}
layer {
  name: "Transform: transform-127-tf.split_3sts0"
  type: "Transform"
  bottom: "tf.split_3sts0:0"
  top: "transform-127-tf.split_3sts0:0"
    scale_param {
    "InputShapes": "7x7x3"
    "OutputShape": "7x7x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "1.53KB"
    "ScheduleId": "758"

  }

}
layer {
  name: "Transform: transform-126-tf.split_3sts1"
  type: "Transform"
  bottom: "tf.split_3sts1:0"
  top: "transform-126-tf.split_3sts1:0"
    scale_param {
    "InputShapes": "7x7x32"
    "OutputShape": "7x7x32"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "1.53KB"
    "ScheduleId": "763"

  }

}
layer {
  name: "Softmax: softmax_2"
  type: "Softmax"
  bottom: "transform-70-tf.reshape_39:0"
  top: "softmax_2:0"
    scale_param {
    "InputShapes": "169x4x8"
    "OutputShape": "169x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "6.00KB"
    "ScheduleId": "795"

  }

}
layer {
  name: "Reshape: tf.reshape_43"
  type: "Reshape"
  bottom: "transform-127-tf.split_3sts0:0"
  top: "tf.reshape_43:0"
    scale_param {
    "InputShapes": "7x7x3"
    "OutputShape": "49x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "2.00KB"
    "ScheduleId": "759"

  }

}
layer {
  name: "Reshape: tf.reshape_42"
  type: "Reshape"
  bottom: "transform-126-tf.split_3sts1:0"
  top: "tf.reshape_42:0"
    scale_param {
    "InputShapes": "7x7x32"
    "OutputShape": "49x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "7.00KB"
    "ScheduleId": "764"

  }

}
layer {
  name: "Transform: transform-53-softmax_2"
  type: "Transform"
  bottom: "softmax_2:0"
  top: "transform-53-softmax_2:0"
    scale_param {
    "InputShapes": "169x4x8"
    "OutputShape": "169x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "42.25KB"
    "ScheduleId": "796"

  }

}
layer {
  name: "Transform: transform-52-tf.reshape_43"
  type: "Transform"
  bottom: "tf.reshape_43:0"
  top: "transform-52-tf.reshape_43:0"
    scale_param {
    "InputShapes": "49x3"
    "OutputShape": "49x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "192B"
    "ScheduleId": "779"

  }

}
layer {
  name: "Transform: transform-71-tf.reshape_42"
  type: "Transform"
  bottom: "tf.reshape_42:0"
  top: "transform-71-tf.reshape_42:0"
    scale_param {
    "InputShapes": "49x4x8"
    "OutputShape": "49x4x8"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.00KB"
    "ScheduleId": "800"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_111"
  type: "Convolution"
  bottom: "transform-53-softmax_2:0"
  top: "keras_quantization_wrapper_111:0"
    scale_param {
    "InputShapes": "169x4x8, 1x1x8x1, 1"
    "OutputShape": "169x4x1"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_111_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_111_bias:0"
    "RuntimeMemory": "5.28KB"
    "ScheduleId": "797"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 8
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Concat: bb_dec_class"
  type: "Concat"
  bottom: "transform-52-tf.reshape_43:0"
  bottom: "transform-49-tf.reshape_34:0"
  bottom: "transform-51-tf.reshape_40:0"
  bottom: "transform-50-tf.reshape_37:0"
  top: "bb_dec_class:0"
    scale_param {
    "InputShapes": "2704x3, 676x3, 169x3, 49x3"
    "OutputShape": "3598x3"
    "Quantize[mn,mx]": [-16.0, 16.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "10.59KB"
    "ScheduleId": "780"

  }

}
layer {
  name: "Softmax: softmax_3"
  type: "Softmax"
  bottom: "transform-71-tf.reshape_42:0"
  top: "softmax_3:0"
    scale_param {
    "InputShapes": "49x4x8"
    "OutputShape": "49x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "2.00KB"
    "ScheduleId": "801"

  }

}
layer {
  name: "Squeeze: tf.compat.v1.squeeze_2"
  type: "Squeeze"
  bottom: "keras_quantization_wrapper_111:0"
  top: "tf.compat.v1.squeeze_2:0"
    scale_param {
    "InputShapes": "169x4x1"
    "OutputShape": "169x4"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "5.28KB"
    "ScheduleId": "799"

  }

}
layer {
  name: "Sigmoid: tf.math.sigmoid"
  type: "Sigmoid"
  bottom: "bb_dec_class:0"
  top: "tf.math.sigmoid:0"
    scale_param {
    "InputShapes": "3598x3"
    "OutputShape": "3598x3"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "10.59KB"
    "ScheduleId": "781"

  }

}
layer {
  name: "Transform: transform-54-softmax_3"
  type: "Transform"
  bottom: "softmax_3:0"
  top: "transform-54-softmax_3:0"
    scale_param {
    "InputShapes": "49x4x8"
    "OutputShape": "49x4x8"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "12.25KB"
    "ScheduleId": "802"

  }

}
layer {
  name: "Transform: transform-92-tf.compat.v1.squeeze_2"
  type: "Transform"
  bottom: "tf.compat.v1.squeeze_2:0"
  top: "transform-92-tf.compat.v1.squeeze_2:0"
    scale_param {
    "InputShapes": "169x4"
    "OutputShape": "169x4"
    "Quantize[mn,mx]": [0, 8.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "768B"
    "ScheduleId": "827"

  }

}
layer {
  name: "Transform: transform-89-tf.math.sigmoid"
  type: "Transform"
  bottom: "tf.math.sigmoid:0"
  top: "transform-89-tf.math.sigmoid:0"
    scale_param {
    "InputShapes": "3598x3"
    "OutputShape": "3598x3"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "10.59KB"
    "ScheduleId": "806"

  }

}
layer {
  name: "Conv2D: keras_quantization_wrapper_113"
  type: "Convolution"
  bottom: "transform-54-softmax_3:0"
  top: "keras_quantization_wrapper_113:0"
    scale_param {
    "InputShapes": "49x4x8, 1x1x8x1, 1"
    "OutputShape": "49x4x1"
    "Quantize[mn,mx]": [0, 4.0]
    "QuantizeBits": 8
    "ConstInputs": "keras_quantization_wrapper_113_kernel:0"
    "ConstInputs": "keras_quantization_wrapper_113_bias:0"
    "RuntimeMemory": "1.53KB"
    "ScheduleId": "803"

  }
  blobs {
    shape {
      dim: 1
      dim: 1
      dim: 8
      dim: 1
    }
  }
  blobs {
    shape {
      dim: 1
    }
  }
}
layer {
  name: "Mul: tf.math.multiply_2"
  type: "Mul"
  bottom: "transform-92-tf.compat.v1.squeeze_2:0"
  top: "tf.math.multiply_2:0"
    scale_param {
    "InputShapes": "169x4, 169x1"
    "OutputShape": "169x4"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.multiply_2_y:0"
    "RuntimeMemory": "768B"
    "ScheduleId": "828"

  }
  blobs {
    shape {
      dim: 169
      dim: 1
    }
  }
}
layer {
  name: "Squeeze: tf.compat.v1.squeeze_3"
  type: "Squeeze"
  bottom: "keras_quantization_wrapper_113:0"
  top: "tf.compat.v1.squeeze_3:0"
    scale_param {
    "InputShapes": "49x4x1"
    "OutputShape": "49x4"
    "Quantize[mn,mx]": [0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "1.53KB"
    "ScheduleId": "805"

  }

}
layer {
  name: "Unstack: tf.unstack_2"
  type: "Unstack"
  bottom: "tf.math.multiply_2:0"
  top: "tf.unstack_2:3:3"
  top: "tf.unstack_2:2:2"
  top: "tf.unstack_2:1:1"
  top: "tf.unstack_2:0"
    scale_param {
    "InputShapes": "169x4"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "768B"
    "ScheduleId": "829"

  }

}
layer {
  name: "Transform: transform-93-tf.compat.v1.squeeze_3"
  type: "Transform"
  bottom: "tf.compat.v1.squeeze_3:0"
  top: "transform-93-tf.compat.v1.squeeze_3:0"
    scale_param {
    "InputShapes": "49x4"
    "OutputShape": "49x4"
    "Quantize[mn,mx]": [0, 4.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "256B"
    "ScheduleId": "837"

  }

}
layer {
  name: "Add: tf.math.add_4"
  type: "Add"
  bottom: "tf.unstack_2:2:2"
  top: "re_lu_10:0"
    scale_param {
    "InputShapes": "169, 169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_4_x:0"
    "RuntimeMemory": "192B"
    "ScheduleId": "830"

  }
  blobs {
    shape {
      dim: 169
    }
  }
}
layer {
  name: "Relu: re_lu_10"
  type: "Relu"
  bottom: "re_lu_10:0"
  top: "re_lu_10:0"
    scale_param {
    "InputShapes": "169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_4_x:0"
    "RuntimeMemory": "192B"
    
  }

}
layer {
  name: "Add: tf.math.add_5"
  type: "Add"
  bottom: "tf.unstack_2:3:3"
  top: "re_lu_11:0"
    scale_param {
    "InputShapes": "169, 169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_5_x:0"
    "RuntimeMemory": "192B"
    "ScheduleId": "831"

  }
  blobs {
    shape {
      dim: 169
    }
  }
}
layer {
  name: "Relu: re_lu_11"
  type: "Relu"
  bottom: "re_lu_11:0"
  top: "re_lu_11:0"
    scale_param {
    "InputShapes": "169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_5_x:0"
    "RuntimeMemory": "192B"
    
  }

}
layer {
  name: "Sub: tf.math.subtract_4"
  type: "Sub"
  bottom: "tf.unstack_2:0"
  top: "re_lu_8:0"
    scale_param {
    "InputShapes": "169, 169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_4_x:0"
    "RuntimeMemory": "192B"
    "ScheduleId": "832"

  }
  blobs {
    shape {
      dim: 169
    }
  }
}
layer {
  name: "Relu: re_lu_8"
  type: "Relu"
  bottom: "re_lu_8:0"
  top: "re_lu_8:0"
    scale_param {
    "InputShapes": "169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_4_x:0"
    "RuntimeMemory": "192B"
    
  }

}
layer {
  name: "Sub: tf.math.subtract_5"
  type: "Sub"
  bottom: "tf.unstack_2:1:1"
  top: "re_lu_9:0"
    scale_param {
    "InputShapes": "169, 169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_5_x:0"
    "RuntimeMemory": "192B"
    "ScheduleId": "833"

  }
  blobs {
    shape {
      dim: 169
    }
  }
}
layer {
  name: "Relu: re_lu_9"
  type: "Relu"
  bottom: "re_lu_9:0"
  top: "re_lu_9:0"
    scale_param {
    "InputShapes": "169"
    "OutputShape": "169"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_5_x:0"
    "RuntimeMemory": "192B"
    
  }

}
layer {
  name: "Mul: tf.math.multiply_3"
  type: "Mul"
  bottom: "transform-93-tf.compat.v1.squeeze_3:0"
  top: "tf.math.multiply_3:0"
    scale_param {
    "InputShapes": "49x4, 49x1"
    "OutputShape": "49x4"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.multiply_3_y:0"
    "RuntimeMemory": "256B"
    "ScheduleId": "838"

  }
  blobs {
    shape {
      dim: 49
      dim: 1
    }
  }
}
layer {
  name: "Stack: tf.stack_2"
  type: "Stack"
  bottom: "re_lu_11:0"
  bottom: "re_lu_10:0"
  bottom: "re_lu_9:0"
  bottom: "re_lu_8:0"
  top: "tf.stack_2:0"
    scale_param {
    "InputShapes": "169, 169, 169, 169"
    "OutputShape": "169x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "768B"
    "ScheduleId": "834"

  }

}
layer {
  name: "Unstack: tf.unstack_3"
  type: "Unstack"
  bottom: "tf.math.multiply_3:0"
  top: "tf.unstack_3:3:3"
  top: "tf.unstack_3:2:2"
  top: "tf.unstack_3:1:1"
  top: "tf.unstack_3:0"
    scale_param {
    "InputShapes": "49x4"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "256B"
    "ScheduleId": "839"

  }

}
layer {
  name: "Reshape: tf.expand_dims_2_reshaped"
  type: "Reshape"
  bottom: "tf.stack_2:0"
  top: "tf.expand_dims_2_reshaped:0"
    scale_param {
    "InputShapes": "169x4"
    "OutputShape": "169x1x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "768B"
    "ScheduleId": "835"

  }

}
layer {
  name: "Sub: tf.math.subtract_6"
  type: "Sub"
  bottom: "tf.unstack_3:0"
  top: "re_lu_12:0"
    scale_param {
    "InputShapes": "49, 49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_6_x:0"
    "RuntimeMemory": "64B"
    "ScheduleId": "842"

  }
  blobs {
    shape {
      dim: 49
    }
  }
}
layer {
  name: "Relu: re_lu_12"
  type: "Relu"
  bottom: "re_lu_12:0"
  top: "re_lu_12:0"
    scale_param {
    "InputShapes": "49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_6_x:0"
    "RuntimeMemory": "64B"
    
  }

}
layer {
  name: "Sub: tf.math.subtract_7"
  type: "Sub"
  bottom: "tf.unstack_3:1:1"
  top: "re_lu_13:0"
    scale_param {
    "InputShapes": "49, 49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 0.5]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.subtract_7_x:0"
    "RuntimeMemory": "64B"
    "ScheduleId": "843"

  }
  blobs {
    shape {
      dim: 49
    }
  }
}
layer {
  name: "Relu: re_lu_13"
  type: "Relu"
  bottom: "re_lu_13:0"
  top: "re_lu_13:0"
    scale_param {
    "InputShapes": "49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 0.5]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.subtract_7_x:0"
    "RuntimeMemory": "64B"
    
  }

}
layer {
  name: "Add: tf.math.add_6"
  type: "Add"
  bottom: "tf.unstack_3:2:2"
  top: "re_lu_14:0"
    scale_param {
    "InputShapes": "49, 49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_6_x:0"
    "RuntimeMemory": "64B"
    "ScheduleId": "840"

  }
  blobs {
    shape {
      dim: 49
    }
  }
}
layer {
  name: "Relu: re_lu_14"
  type: "Relu"
  bottom: "re_lu_14:0"
  top: "re_lu_14:0"
    scale_param {
    "InputShapes": "49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_6_x:0"
    "RuntimeMemory": "64B"
    
  }

}
layer {
  name: "Add: tf.math.add_7"
  type: "Add"
  bottom: "tf.unstack_3:3:3"
  top: "re_lu_15:0"
    scale_param {
    "InputShapes": "49, 49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 16
    "ConstInputs": "tf.math.add_7_x:0"
    "RuntimeMemory": "64B"
    "ScheduleId": "841"

  }
  blobs {
    shape {
      dim: 49
    }
  }
}
layer {
  name: "Relu: re_lu_15"
  type: "Relu"
  bottom: "re_lu_15:0"
  top: "re_lu_15:0"
    scale_param {
    "InputShapes": "49"
    "OutputShape": "49"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "ConstInputs": "tf.math.add_7_x:0"
    "RuntimeMemory": "64B"
    
  }

}
layer {
  name: "Stack: tf.stack_3"
  type: "Stack"
  bottom: "re_lu_13:0"
  bottom: "re_lu_15:0"
  bottom: "re_lu_14:0"
  bottom: "re_lu_12:0"
  top: "tf.stack_3:0"
    scale_param {
    "InputShapes": "49, 49, 49, 49"
    "OutputShape": "49x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "256B"
    "ScheduleId": "844"

  }

}
layer {
  name: "Reshape: tf.expand_dims_3_reshaped"
  type: "Reshape"
  bottom: "tf.stack_3:0"
  top: "tf.expand_dims_3_reshaped:0"
    scale_param {
    "InputShapes": "49x4"
    "OutputShape": "49x1x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    "HasScratch": "true"
    
    "RuntimeMemory": "256B"
    "ScheduleId": "845"

  }

}
layer {
  name: "Concat: bb_dec_bbox"
  type: "Concat"
  bottom: "tf.expand_dims_1_reshaped:0"
  bottom: "tf.expand_dims_reshaped:0"
  bottom: "tf.expand_dims_3_reshaped:0"
  bottom: "tf.expand_dims_2_reshaped:0"
  top: "bb_dec_bbox:0"
    scale_param {
    "InputShapes": "2704x1x4, 676x1x4, 169x1x4, 49x1x4"
    "OutputShape": "3598x1x4"
    "Quantize[mn,mx]": [0, 2.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "14.13KB"
    "ScheduleId": "847"

  }

}
layer {
  name: "Cast: tf.image.combined_non_max_suppression_iq0"
  type: "Cast"
  bottom: "bb_dec_bbox:0"
  top: "tf.image.combined_non_max_suppression_iq0:0"
    scale_param {
    "InputShapes": "3598x1x4"
    "OutputShape": "3598x1x4"
    "Quantize[mn,mx]": [-2.0, 2.0]
    "QuantizeBits": 16
    
    "RuntimeMemory": "28.13KB"
    "ScheduleId": "848"

  }

}
layer {
  name: "Transform: transform-88-tf.image.combined_non_max_suppression_iq0"
  type: "Transform"
  bottom: "tf.image.combined_non_max_suppression_iq0:0"
  top: "transform-88-tf.image.combined_non_max_suppression_iq0:0"
    scale_param {
    "InputShapes": "3598x1x4"
    "OutputShape": "3598x1x4"
    "Quantize[mn,mx]": [-2.0, 2.0]
    "QuantizeBits": 16
    
    "RuntimeMemory": "28.13KB"
    "ScheduleId": "849"

  }

}
layer {
  name: "MultiClassNms: tf.image.combined_non_max_suppression"
  type: "MultiClassNms"
  bottom: "transform-89-tf.math.sigmoid:0"
  bottom: "transform-88-tf.image.combined_non_max_suppression_iq0:0"
  top: "tf.image.combined_non_max_suppression:3:3"
  top: "tf.image.combined_non_max_suppression:2:2"
  top: "tf.image.combined_non_max_suppression:1:1"
  top: "tf.image.combined_non_max_suppression:0"
    scale_param {
    "InputShapes": "3598x1x4, 3598x3"
    "OutputShape": "300x4"
    "Quantize[mn,mx]": [-2.0, 2.0]
    "QuantizeBits": 16
    "HasScratch": "true"
    
    "RuntimeMemory": "3.31KB"
    "ScheduleId": "850"

  }

}
layer {
  name: "Transform: transform-31-tf.image.combined_non_max_suppression"
  type: "Transform"
  bottom: "tf.image.combined_non_max_suppression:0"
  top: "transform-31-tf.image.combined_non_max_suppression:0"
    scale_param {
    "InputShapes": "300x4"
    "OutputShape": "300x4"
    "Quantize[mn,mx]": [-2.0, 2.0]
    "QuantizeBits": 16
    
    "RuntimeMemory": "18.75KB"
    "ScheduleId": "852"

  }

}
layer {
  name: "Transform: transform-32-tf.image.combined_non_max_suppression"
  type: "Transform"
  bottom: "tf.image.combined_non_max_suppression:1:1"
  top: "transform-32-tf.image.combined_non_max_suppression:0"
    scale_param {
    "InputShapes": "300"
    "OutputShape": "300"
    "Quantize[mn,mx]": [0, 1.0]
    "QuantizeBits": 8
    
    "RuntimeMemory": "320B"
    "ScheduleId": "854"

  }

}
layer {
  name: "Transform: transform-33-tf.image.combined_non_max_suppression"
  type: "Transform"
  bottom: "tf.image.combined_non_max_suppression:2:2"
  top: "transform-33-tf.image.combined_non_max_suppression:0"
    scale_param {
    "InputShapes": "300"
    "OutputShape": "300"
    "Quantize[mn,mx]": [-32768.0, 32768.0]
    "QuantizeBits": 16
    
    "RuntimeMemory": "608B"
    "ScheduleId": "856"

  }

}
layer {
  name: "Transform: transform-34-tf.image.combined_non_max_suppression"
  type: "Transform"
  bottom: "tf.image.combined_non_max_suppression:3:3"
  top: "transform-34-tf.image.combined_non_max_suppression:0"
    scale_param {
    "InputShapes": "1"
    "OutputShape": "1"
    "Quantize[mn,mx]": [-32768.0, 32768.0]
    "QuantizeBits": 16
    
    "RuntimeMemory": "32B"
    "ScheduleId": "858"

  }

}
layer {
  name: "Output: Output0"
  type: "Output"
  bottom: "transform-31-tf.image.combined_non_max_suppression:0"
  
    scale_param {
    "InputShapes": "300x4"
    
    
    
    "RuntimeMemory": "0B"
    "ScheduleId": "853"

  }

}
layer {
  name: "Output: output_sync"
  type: "Output"
  bottom: "input_1:0"
  bottom: "transform-34-tf.image.combined_non_max_suppression:0"
  bottom: "transform-33-tf.image.combined_non_max_suppression:0"
  bottom: "transform-32-tf.image.combined_non_max_suppression:0"
  bottom: "transform-31-tf.image.combined_non_max_suppression:0"
  
    scale_param {
    "InputShapes": "300x4, 300, 300, 1, 416x416x3"
    
    
    
    "RuntimeMemory": "0B"
    "ScheduleId": "860"

  }

}
layer {
  name: "Output: Output1"
  type: "Output"
  bottom: "transform-32-tf.image.combined_non_max_suppression:0"
  
    scale_param {
    "InputShapes": "300"
    
    
    
    "RuntimeMemory": "0B"
    "ScheduleId": "855"

  }

}
layer {
  name: "Output: Output2"
  type: "Output"
  bottom: "transform-33-tf.image.combined_non_max_suppression:0"
  
    scale_param {
    "InputShapes": "300"
    
    
    
    "RuntimeMemory": "0B"
    "ScheduleId": "857"

  }

}
layer {
  name: "Output: Output3"
  type: "Output"
  bottom: "transform-34-tf.image.combined_non_max_suppression:0"
  
    scale_param {
    "InputShapes": "1"
    
    
    
    "RuntimeMemory": "0B"
    "ScheduleId": "859"

  }

}
layer{
  name: "nanodet-quant-ppe"
  type: "MemoryReport"
  scale_param {
   
    RuntimeMemoryPhysicalSize: "3.16MB"
    ModelMemoryPhysicalSize: "2.69MB"
    ReservedMemory: "1.00KB"
    MemoryUsage: "5.85MB"
    TotalMemoryAvailableOnChip: "8.00MB"
    MemoryUtilization: "74%"
    FitInChip: "true"
    InputPersistent: "true"
    Hash: "nanodet-quant-ppe"
    InputPersistenceCost: "511.00KB"
      
   LargestLayer: "zero_padding2d_1_to1d_1"
  }
}
